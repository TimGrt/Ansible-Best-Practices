{"config":{"lang":["en"],"separator":"[\\s]","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Best Practices","text":"<p>This document aims to gather good and best practices from Ansible practitioners and experience from multiple Ansible projects. It strives to give all Ansible users a guideline from which to start their automation journey in good conditions.</p> <p> </p> <p>Ansible is simple, flexible, and powerful. Like any powerful tool, there are many ways to use it, some better than others.</p> <p>Those are opinionated guidelines based on the experience of many projects. They are not meant to be followed blindly if they don\u2019t fit the reader\u2019s specific use case or needs. Take them as an inspiration and adjust them to your needs, still let us know your good and best practices, we all can learn.</p> <ul> <li> Mindset</li> <li> Ansible</li> <li> Ansible Development</li> <li> Ansible Automation Platform</li> </ul> <p>Searching for something specific? Use the Search at the top!</p> <p>Versioning</p> <p>This guide is updated constantly, last update on February 17, 2025.</p>"},{"location":"ansible/","title":"Ansible","text":"<p>This topic is split into seven main sections, each section covers a different aspect of automation using Ansible.</p> <ul> <li> <p> \u00a0 Installation</p> <p>How to install Ansible and run it, from present to future.</p> </li> <li> <p> \u00a0 Project</p> <p>Your Ansible project, versioning control, dependencies, syntax</p> </li> <li> <p> \u00a0 Inventory</p> <p>How to define your inventory and target hosts</p> </li> <li> <p> \u00a0 Playbook</p> <p>Structure your automation, how to separate playbooks and plays</p> </li> <li> <p> \u00a0 Roles</p> <p>A best practice in itself, including how to create and fill the role folder</p> </li> <li> <p> \u00a0 Tasks</p> <p>Everything about tasks, module usage, tags, loops and filters</p> </li> <li> <p> \u00a0 Variables</p> <p>All about variables, where to store them, naming conventions and encryption</p> </li> </ul>"},{"location":"ansible/installation/","title":"Installation","text":""},{"location":"ansible/installation/#standard-install-method","title":"Standard install method","text":"<p>The latest version can only be obtained via the Python package manager, the ansible-core package contains the binaries and 69 standard modules.</p> <pre><code>pip3 install ansible-core\n</code></pre> <p>The included modules can be listed with <code>ansible-doc --list ansible.builtin</code>. If more special modules are needed, the complete ansible package can be installed, this corresponds to the \"old\" installation method (batteries included).</p> <pre><code>pip3 install ansible\n</code></pre> <p>Tip</p> <p>It makes sense to install only the ansible-core package. Afterwards, install the few collections necessary for your project via <code>ansible-galaxy</code>. This way you have an up-to-date, lean installation without unnecessary modules and plugins. Take a look at the following section for the recommended installation.</p> <p>Most OS package managers like apt or yum also provide the <code>ansible-core</code> or <code>ansible</code> packages, these versions are not latest but a couple of minor versions behind.</p> Installing Ansible with OS package manager <p>Even in fairly recent distributions the Ansible versions are not up to date:</p> <pre><code>$ pip3 show ansible-core\nName: ansible-core\nVersion: 2.14.3\n...\n</code></pre> <pre><code>$ dnf info ansible-core\nAvailable Packages\nName         : ansible-core\nVersion      : 2.13.3\nRelease      : 2.el8_7\nArchitecture : x86_64\nSize         : 2.8 M\nSource       : ansible-core-2.13.3-2.el8_7.src.rpm\nRepository   : appstream\n...\n</code></pre> <pre><code>$ apt info ansible-core\nPackage: ansible-core\nVersion: 2.12.0-1ubuntu0.1\nPriority: optional\nSection: universe/admin\nOrigin: Ubuntu\n...\n</code></pre>"},{"location":"ansible/installation/#install-collections","title":"Install Collections","text":"<p>The recommended installation method is through the Python package manager, necessary modules and plugins not included in the <code>ansible-core</code> binary are installed through collections. Additional collections (the included collection is called ansible.builtin) are installed with the <code>ansible-galaxy</code> command-line utility:</p> <pre><code>ansible-galaxy collection install community.general\n</code></pre> <p>Multiple collections can be installed at once with a <code>requirements.yml</code> file.</p> <p>Thereby the chapter Project &gt; Collections is to be considered. If a container runtime is available, the complete installation can also be bundled in a container image (so-called Execution Environment).</p> <p>By default, collections are installed into a (hidden) folder in the home directory (<code>~/.ansible/collections/ansible_collections/</code>). This is defined by the <code>collections_path</code> configuration setting.</p> <p>If you want to store collections alongside you project, create a folder <code>collections</code> in your project directory and install collections by providing the <code>--collections-path</code> (<code>-p</code>) argument:</p> <pre><code>ansible-galaxy collection install community.general --collections-path ./collections/\n</code></pre>"},{"location":"ansible/installation/#list-installed-collections","title":"List installed collections","text":"<p>Show the name and version of each collection installed in the <code>collections_path</code>:</p> <pre><code>ansible-galaxy collection list\n</code></pre>"},{"location":"ansible/installation/#upgrade-installed-collections","title":"Upgrade installed collections","text":"<p>To upgrade installed collections use the <code>--upgrade</code> (<code>-U</code>) argument:</p> <pre><code>ansible-galaxy collection install community.general --upgrade\n</code></pre>"},{"location":"ansible/installation/#install-collections-offline","title":"Install collections offline","text":"<p>Download the collection tarball from Galaxy for offline use:</p> <ol> <li>Navigate to the collection page.</li> <li>Click on Download tarball.</li> <li>Copy the archive to the remote server.</li> <li> <p>Install the collection with the <code>ansible-galaxy</code> CLI utility, use the <code>--offline</code> argument:</p> <pre><code>ansible-galaxy collection install ~/community-general-6.4.0.tar.gz --offline\n</code></pre> </li> </ol>"},{"location":"ansible/installation/#execution-environments","title":"Execution environments","text":"<p>Execution Environments are container images that serve as Ansible control nodes. EEs provide you with:</p> <ul> <li>Software dependency isolation</li> <li>Portability across teams and environments</li> <li>Separation from other automation content and tooling</li> </ul>"},{"location":"ansible/installation/#ansible-builder","title":"Ansible Builder","text":"<p>Ansible Builder is a tool that aids in the creation of Ansible Execution Environments. It does this by using the dependency information defined in various Ansible Content Collections, as well as by the user. Ansible Builder will produce a directory that acts as the build context for the container image build, which will contain the Containerfile (Dockerfile), along with any other files that need to be added to the image. There is no need to write a single line of Dockerfile, which makes it easy to build and use Execution Environments.</p> <p>To build an EE, install <code>ansible-builder</code> from the Python Package Manager:</p> <pre><code>pip3 install ansible-builder\n</code></pre> <p>Define at least the definition file for the Execution Environment and other files, depending on your use-case.</p> EE definition fileCollection DependenciesPython DependenciesCross-Platform requirements <p>execution-environment.yml</p> <pre><code>---\nversion: 3\n\nimages:\n  base_image: # (1)!\n    name: ghcr.io/ansible-community/community-ee-base:latest\n\ndependencies: # (2)!\n  galaxy: requirements.yml # (3)!\n  python: requirements.txt # (4)!\n  system: bindep.txt\n</code></pre> <ol> <li>Some more useful base images are (take a look if a more recent tag is available):<ul> <li>quay.io/rockylinux/rockylinux:9</li> <li>ghcr.io/ansible-community/community-ee-minimal:latest</li> <li>registry.redhat.io/ansible-automation-platform-24/ee-supported-rhel9:1.0.0-456</li> <li>registry.redhat.io/ansible-automation-platform/ee-minimal-rhel9::2.15.5-4</li> </ul> </li> <li>If you want to install a specific Ansible version add this configuration under the <code>dependencies</code> key: <pre><code>dependencies:\n  ansible_core:\n    package_pip: ansible-core==2.14.3\n</code></pre></li> <li>Instead of using a separate file, you can provide collections (and roles) as a list: <pre><code>dependencies:\n  galaxy:\n    collections:\n      - kubernetes.core\n    roles:\n      - timgrt.terraform\n</code></pre></li> <li>Instead of using a separate file, you can provide the Python packages as a list: <pre><code>dependencies:\n  python:\n    - awxkit\n    - boto\n    - botocore\n    - boto3\n    - openshift\n    - requests-oauthlib\n</code></pre></li> </ol> <p>requirements.yml</p> <pre><code>---\ncollections:\n  - redhat.openshift\n</code></pre> <p>requirements.txt</p> <pre><code>awxkit&gt;=13.0.0\nboto&gt;=2.49.0\nbotocore&gt;=1.12.249\nboto3&gt;=1.9.249\nopenshift&gt;=0.6.2\nrequests-oauthlib\n</code></pre> <p>bindep.txt</p> <p>If there are RPMS necessary, put them here. <pre><code>subversion [platform:rpm]\nsubversion [platform:dpkg]\n</code></pre></p> Package manager not found? <p>In case you see an error like this: <code>unable to execute /usr/bin/dnf: No such file or directory</code>. This can happen when using RHEL minimal images, you need to adjust the package manager path. Add the following setting to your <code>execution-environment.yml</code>:</p> <pre><code>options:\n  package_manager_path: /usr/bin/microdnf\n</code></pre> <p>For more information, go to the Ansible Builder Documentation.</p> <p>To build the EE, run this command (assuming you have Docker installed, by default Podman is used):</p> <pre><code>ansible-builder build --tag=demo/openshift-ee --container-runtime=docker -v=3\n</code></pre> <p>The resulting container images can be viewed with the <code>docker images</code> command:</p> <pre><code>$ docker images\nREPOSITORY                        TAG       IMAGE ID       CREATED              SIZE\ndemo/openshift-ee                 latest    2ea9d5d7b185   10 seconds ago       1.14GB\n</code></pre> <p>You can also build Execution Environments with ansible-navigator, the Builder is installed alongside Navigator.</p> <pre><code>ansible-navigator builder build --tag=demo/openshift-ee --container-runtime=docker\n</code></pre>"},{"location":"ansible/installation/#ansible-runner","title":"Ansible Runner","text":"<p>Using the EE requires a binary which can make use of the Container images, it is not possible to run them with the <code>ansible-playbook</code> binary. You have to use (and install) either the <code>ansible-navigator</code> or the <code>ansible-runner</code> binary.</p> <p>Tip</p> <p>The Ansible Navigator is easier to use than the <code>ansible-runner</code>, use this one for creating, reviewing, running and troubleshooting Ansible content, including inventories, playbooks, collections, documentation and execution environments.  </p> <p>Ansible Runner is a tool and python library to provide a stable and consistent interface abstraction to Ansible, it represents the modularization of the part of Ansible AWX that is responsible for running <code>ansible</code> and <code>ansible-playbook</code> tasks and gathers the output from it.</p> <p>If you want to use it standalone, install the <code>ansible-runner</code> binary:</p> <pre><code>pip3 install ansible-runner\n</code></pre> <p>To use the Ansible from the container image, e.g. run this command which executes an ad hoc command (setup module) against localhost:</p> <pre><code>ansible-runner run --container-image demo/openshift-ee /tmp -m setup --hosts localhost\n</code></pre> <p>Most parameters should be self-explanatory:</p> <ul> <li>run - Run ansible-runner in the foreground</li> <li>--container-image demo/openshift - Container image to use when running an ansible task</li> <li>/tmp - base directory containing the ansible-runner metadata (project, inventory, env, etc)</li> <li>-m setup - Module to execute</li> <li>--hosts localhost - set of hosts to execute against (here only localhost)</li> </ul> <p>The output looks like expected:</p> <pre><code>$ ansible-runner run --container-image demo/openshift-ee /tmp -m setup --hosts localhost\n[WARNING]: No inventory was parsed, only implicit localhost is available\nlocalhost | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"ansible_all_ipv4_addresses\": [\n            \"192.168.178.114\",\n            \"172.17.0.1\"\n        ],\n        \"ansible_all_ipv6_addresses\": [\n            \"2001:9e8:4a14:2401:a00:27ff:febf:4207\",\n            \"fe80::a00:27ff:febf:4207\",\n            \"fe80::42:9eff:fef9:df59\"\n        ],\n        \"ansible_apparmor\": {\n            \"status\": \"enabled\"\n        },\n        \"ansible_architecture\": \"x86_64\",\n        \"ansible_bios_date\": \"12/01/2006\",\n        \"ansible_bios_vendor\": \"innotek GmbH\",\n        \"ansible_bios_version\": \"VirtualBox\",\n        \"ansible_board_asset_tag\": \"NA\",\n        \"ansible_board_name\": \"VirtualBox\",\n        \"ansible_board_serial\": \"NA\",\n        \"ansible_board_vendor\": \"Oracle Corporation\",\n        ...\n</code></pre>"},{"location":"ansible/installation/#ansible-navigator","title":"Ansible Navigator","text":"<p>The <code>ansible-navigator</code> is text-based user interface (TUI) for the Red Hat Ansible Automation Platform. The Navigator also makes use of the Execution Environments and provides an easier to use interface to interact with EEs (than ansible-runner). Install the <code>ansible-navigator</code> binary and its dependencies with the Python package manager:</p> <pre><code>pip3 install ansible-navigator\n</code></pre> <p>If you want to use the Navigator with EEs, you'll need a container runtime, install Docker or Podman an your system.</p> <p>With the Navigator you, for example, can inspect *all locally available Execution Environments</p> <p>Take a look at the Playbooks section on how to run playbooks in Execution Environments with the Navigator.</p> <p>Some <code>ansible-navigator</code> commands map to <code>ansible</code> commands (prefix every Navigator command with <code>ansible-navigator</code>):</p> Navigator command Description <code>exec -- ansible ...</code> Runs Ansible ad-hoc commands. <code>builder</code> Builds new execution environments, the <code>ansible-builder</code> utility is installed with <code>ansible-navigator</code>. <code>config</code> Explore the current ansible configuration as with <code>ansible-config</code>. <code>doc</code> Explore the documentation for modules and plugins as with <code>ansible-doc</code>. <code>inventory</code> Inspect the inventory and browser groups and hosts. <code>lint</code> Runs best-practice checker, <code>ansible-lint</code> needs to be installed locally or in the selected execution-environment. <code>run</code> Runs Playbooks. <code>exec -- ansible-test ...</code> Executes sanity, unit and integration tests for Collections. <code>exec -- ansible-vault ...</code> Runs utility to encrypt or decrypt Ansible content."},{"location":"ansible/inventory/","title":"Inventory","text":"<p>An inventory is a list of managed nodes, or hosts, that Ansible deploys and configures. The inventory can either be static or dynamic.</p>"},{"location":"ansible/inventory/#convert-ini-to-yaml","title":"Convert INI to YAML","text":"<p>The most common format for the Ansible Inventory is the <code>.ini</code> format, but sometimes you might need the inventory file in the YAML format.  A <code>.ini</code> inventory file for example might look like this:</p> inventory.ini<pre><code>[control]\ncontroller ansible_host=localhost ansible_connection=local\n\n[target]\nrocky8 ansible_connection=docker\n</code></pre> <p>You can convert your existing inventory to the YAML format with the <code>ansible-inventory</code> utility.</p> <pre><code>ansible-inventory -i inventory.ini -y --list &gt; inventory.yml\n</code></pre> <p>The resulting file is your inventory in YAML format:</p> inventory.yml<pre><code>all:\n  children:\n    control:\n      hosts:\n        controller:\n          ansible_connection: local\n          ansible_host: localhost\n    target:\n      hosts:\n        rocky8:\n          ansible_connection: docker\n</code></pre>"},{"location":"ansible/inventory/#static-inventory","title":"Static inventory","text":"<p>Warning</p> <p>Work in Progress - More description necessary.</p>"},{"location":"ansible/inventory/#dynamic-inventory","title":"Dynamic inventory","text":"<p>Warning</p> <p>Work in Progress - More description necessary.</p>"},{"location":"ansible/inventory/#custom-dynamic-inventory","title":"Custom dynamic inventory","text":"<p>In case no suitable inventory plugin exists, you can easily write your own. Take a look at the Ansible Development - Extending section for additional information.</p>"},{"location":"ansible/inventory/#in-memory-inventory","title":"In-Memory Inventory","text":"<p>Normally Ansible requires an inventory file, to know which machines it is meant to operate on.</p> <p>This is typically a manual process but can be greatly improved by using a dynamic inventory to pull inventory information from other systems.</p> <p>Suppose, however, you needed to create X number of instances, which are transient in nature and had no existing details available to populate an inventory file for Ansible to utilise. If X is a small number, you could easily hand-craft the inventory file while the playbook already runs.</p> <p>Use the <code>add_host</code> module, which makes use of Ansible's ability to populate an in-memory inventory with information it generates while creating new instances.</p> <p>Take a look at the following example, the first play creates a couple of Containers and adds them to a new group. The seconds plays targets this new group and connects to the newly created Containers.</p> <pre><code>---\n- name: Add hosts to additional groups\n  hosts: localhost\n  connection: local\n  gather_facts: false\n  vars:\n    container_list:\n      - node1\n      - node2\n      - node3\n  tasks:\n    - name: Start managed node containers\n      containers.podman.podman_container:\n        name: \"{{ item }}\"\n        image: docker.io/timgrt/rockylinux8-ansible:latest\n        hostname: \"{{ item }}.example.com\"\n        stop_signal: 15\n        state: started\n      loop: \"{{ container_list }}\"\n\n    - name: Add container to new group\n      ansible.builtin.add_host:\n        name: \"{{ item }}\" # (1)!\n        groups: managed_node_containers # (2)!\n        ansible_connection: podman # (3)!\n        ansible_python_interpreter: /usr/libexec/platform-python # (4)!\n        stage: test # (5)!\n      loop: \"{{ container_list }}\"\n\n- name: Run tasks on containers created in previous play\n  hosts: managed_node_containers\n  tasks:\n    - name: Output stage variable\n      ansible.builtin.debug:\n        msg: \"{{ stage }}\"\n</code></pre> <ol> <li>Every container instance is added by looping the variable <code>container_list</code>. As the <code>name</code> parameter must be a string a loop is necessary.</li> <li>This is the name of the new group! It is targeted in the second play. The <code>groups</code> parameter can be a list of multiple group names.</li> <li>These are variables needed to connect to the new instances. As they are Podman containers the podman connection plugin is used.</li> <li>The Python interpreter which is used in the new instances. Not always necessary, as normally Ansible discovers the interpreter pretty reliable.</li> <li>This is a custom variable for all new instances. You can add more variables here if necessary.</li> </ol> Playbook output <pre><code>$ ansible-playbook in-memory-inventory.yml\n\nPLAY [Add hosts to additional groups] *******************************************************************************************************************************\n\nTASK [Start managed node containers] ********************************************************************************************************************************\nok: [localhost] =&gt; (item=node1)\nok: [localhost] =&gt; (item=node2)\nok: [localhost] =&gt; (item=node3)\n\nTASK [Add container to new group] ***********************************************************************************************************************************\nchanged: [localhost] =&gt; (item=node1)\nchanged: [localhost] =&gt; (item=node2)\nchanged: [localhost] =&gt; (item=node3)\n\nPLAY [Run tasks on containers created in previous play] *************************************************************************************************************\n\nTASK [Gathering Facts] **********************************************************************************************************************************************\nok: [node2]\nok: [node1]\nok: [node3]\n\nTASK [Output stage variable] ****************************************************************************************************************************************\nok: [node1] =&gt;\n    msg: test\nok: [node2] =&gt;\n    msg: test\nok: [node3] =&gt;\n    msg: test\n</code></pre>"},{"location":"ansible/playbook/","title":"Playbooks","text":"<p>Playbooks are first thing you think of when using Ansible. This section describes some good practices.  </p>"},{"location":"ansible/playbook/#directory-structure","title":"Directory structure","text":"<p>The main playbook should have a recognizable name, e.g. referencing the projects name or scope. If you have multiple playbooks, create a new folder <code>playbooks</code> and store all playbooks there, except the main playbook (here called <code>site.yml</code>).</p> <pre><code>.\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 site.yml\n\u2514\u2500\u2500 playbooks\n    \u251c\u2500\u2500 database.yml\n    \u251c\u2500\u2500 loadbalancer.yml\n    \u2514\u2500\u2500 webserver.yml\n</code></pre> <p>The <code>site.yml</code> file contains references to the other playbooks:</p> <pre><code>---\n# Main playbook including all other playbooks\n\n- ansible.builtin.import_playbook: playbooks/database.yml # noqa name[play]\n- ansible.builtin.import_playbook: playbooks/webserver.yml # noqa name[play]\n- ansible.builtin.import_playbook: playbooks/loadbalancer.yml # noqa name[play]\n</code></pre> <code>noqa</code> statement <p>The file <code>site.yml</code> only references other playbooks, still, the ansible-lint utility would trigger, as every play should have the <code>name</code> parameter. While this is correct (and you should always name your actual plays), the name parameter on import statements is not shown anyway, as they are pre-processed at the time playbooks are parsed. Take a look at import vs. include in the tasks section </p> <p>Success</p> <p>Therefore, silencing the linter in this particular case with the <code>noqa</code> statement is acceptable.  </p> <p>In contrast, include statements like <code>ansible.builtin.include_tasks</code> should have the <code>name</code> parameter, as these statements are processed when they are encountered during the execution of the playbook.</p> <p>The lower-level playbooks contains actual plays:</p> playbooks/database.yml<pre><code>---\n- name: Install and configure PostgreSQL database\n  hosts: postgres_servers\n  roles:\n    - postgres\n</code></pre> <p>To be able to run the overall playbook, as well as the imported playbooks, add this parameter to your <code>ansible.cfg</code>, otherwise roles are not found:</p> <pre><code>[defaults]\nroles_path = .roles\n</code></pre>"},{"location":"ansible/playbook/#playbook-definition","title":"Playbook definition","text":"<p>Don't put too much logic in your playbook, put it in your roles (or even in custom modules). A playbook could contain <code>pre_tasks</code>, <code>roles</code>, <code>tasks</code> and <code>post_tasks</code> sections, try to limit your playbooks to a list of a roles.</p> <p>Warning</p> <p>Avoid using both roles and tasks sections, the latter possibly containing <code>import_role</code> or <code>include_role</code> tasks. The order of execution between roles and tasks isn\u2019t obvious, and hence mixing them should be avoided.</p> <p>Either you need only static importing of roles and you can use the roles section, or you need dynamic inclusion and you should use only the tasks section. Of course, for very simple cases, you can just use tasks without roles (but playbooks/projects grow quickly, refactor to roles early).</p>"},{"location":"ansible/playbook/#plays","title":"Plays","text":"<p>Avoid putting multiple plays in a playbook, if not really necessary. As every play most likely targets a different host group, create a separate playbook file for it. This way you achieve to most flexibility.</p> k8s_installation.yml<pre><code>---\n- name: Initialize Control-Plane Nodes\n  hosts: kubemaster\n  become: true\n  roles:\n    - k8s_control_plane\n\n- name: Install and configure Worker Nodes\n  hosts: kubeworker\n  become: true\n  roles:\n    - k8s_worker_nodes\n</code></pre> <p>Separate the two plays into their respective playbooks files and reference them in an overall playbook file:</p> k8s_control_plane_playbook.yml<pre><code>---\n- name: Initialize Control-Plane Nodes\n  hosts: kubemaster\n  become: true\n  roles:\n    - k8s_control_plane\n</code></pre> k8s_worker_node_playbook.yml<pre><code>---\n- name: Install and configure Worker Nodes\n  hosts: kubeworker\n  become: true\n  roles:\n    - k8s_worker_nodes\n</code></pre> k8s_installation.yml<pre><code>---\n- ansible.builtin.import_playbook: k8s_control_plane_playbook.yml # noqa name[play]\n- ansible.builtin.import_playbook: k8s_worker_node_playbook.yml # noqa name[play]\n</code></pre>"},{"location":"ansible/playbook/#module-defaults","title":"Module defaults","text":"<p>If your playbook uses modules which need the be called with the same set of parameters or arguments, you can define these as module_defaults. The defaults can be set at play, block or task level.</p> <p>Module defaults are defined by grouping together modules that share common sets of parameters, especially for modules making heavy use of API-interaction such as cloud modules.  </p> <p>Since ansible-core 2.12, collections can define their own groups in the <code>meta/runtime.yml</code> file. module_defaults does not take the collections keyword into account, so the fully qualified group name must be used for new groups in module_defaults.</p> GoodBad <pre><code>---\n- name: Demo play with modules which need to call the same arguments\n  hosts: aci\n  module_defaults:\n    group/cisco.aci.all:\n      host: \"{{ apic_api }}\"\n      username: \"{{ apic_user }}\"\n      password: \"{{ apic_password }}\"\n      validate_certs: false\n  tasks:\n    - name: Get system info\n      cisco.aci.aci_system:\n        state: query\n\n    - name: Create a new demo tenant\n      cisco.aci.aci_tenant:\n        name: demo-tenant\n        description: Tenant for demo purposes\n        state: present\n</code></pre> <p>Authentication parameters are repeated in every task. <pre><code>- name: Demo play with modules which need to call the same arguments\n  hosts: aci\n  tasks:\n    - name: Get system info\n      cisco.aci.aci_system:\n        host: \"{{ apic_api }}\"\n        username: \"{{ apic_user }}\"\n        password: \"{{ apic_password }}\"\n        validate_certs: false\n        state: query\n\n    - name: Create a new demo tenant\n      cisco.aci.aci_tenant:\n        host: \"{{ apic_api }}\"\n        username: \"{{ apic_user }}\"\n        password: \"{{ apic_password }}\"\n        validate_certs: false\n        name: demo-tenant\n        description: Tenant for demo purposes\n        state: present\n</code></pre></p> <p>To identify the correct group (remember, these are not inventory groups), take a look at the <code>meta/runtime.yml</code> of the desired collection. It needs to define the <code>action_groups</code> list, for example:</p> ~/.ansible/collections/ansible_collections/cisco/aci/meta/runtime.yml<pre><code>---\nrequires_ansible: '&gt;=2.9.10'\naction_groups:\n  all:\n    - aci_aaa_custom_privilege\n    - aci_aaa_domain\n    - aci_aaa_role\n    - aci_aaa_ssh_auth\n    - aci_aaa_user\n    - aci_aaa_user_certificate\n    - aci_aaa_user_domain\n    - aci_aaa_user_role\n    - aci_access_port_block_to_access_port\n    ...\n</code></pre> <p>The group is called <code>all</code>, therefore the module defaults groups needs to be <code>group/cisco.aci.all</code>.</p> <p>Note</p> <p>Any module defaults set at the play level (and block/task level when using <code>include_role</code> or <code>import_role</code>) will apply to any roles used, which may cause unexpected behavior in the role.</p>"},{"location":"ansible/playbook/#collections-in-playbooks","title":"Collections in playbooks","text":"<p>In a playbook, you can control the collections Ansible searches for modules and action plugins to execute.</p> <p>tl;dr</p> <p>This is not recommended, try to avoid this.</p> <pre><code>- name: Initialize Control-Plane Nodes\n  hosts: kubemaster\n  collections:\n    - kubernetes.core\n    - computacenter.utils\n  become: true\n  roles:\n    - k8s_control_plane\n</code></pre> <p>With that you could omit the provider.collection part when using modules, by default you would reference a module with the FQCN:</p> <pre><code>- name: Check if Weave is already installed\n  kubernetes.core.k8s_info:\n    api_version: v1\n    kind: DaemonSet\n    name: weave-net\n    namespace: kube-system\n  register: weave_daemonset\n</code></pre> <p>With the <code>collections</code> list defined as part of the play definition, you could write your tasks like this:</p> <pre><code>- name: Check if Weave is already installed\n  k8s_info:\n    api_version: v1\n    kind: DaemonSet\n    name: weave-net\n    namespace: kube-system\n  register: weave_daemonset\n</code></pre> <p>Warning</p> <p>If your playbook uses both the collections keyword and one or more roles, the roles do not inherit the collections set by the playbook! The collections keyword merely creates an ordered search path for non-namespaced plugin and role references. It does not install content or otherwise change Ansible\u2019s behavior around the loading of plugins or roles. Note that an FQCN is still required for non-action or module plugins (for example, lookups, filters, tests).</p> <p>Tip</p> <p>It is preferable to use a module or plugin\u2019s FQCN over the <code>collections</code> keyword!</p>"},{"location":"ansible/playbook/#executing-playbooks","title":"Executing playbooks","text":"<p>To run your playbook, use the <code>ansible-playbook</code> command.</p> <pre><code>ansible-playbook playbook.yml\n</code></pre> <p>Some useful command-line parameters when executing your playbook are the following</p> <ul> <li><code>-C</code> or <code>--check</code> runs the playbook without making any modifications</li> <li><code>-D</code> or <code>--diff</code> shows the differences when changing (small) files and templates</li> <li><code>--step</code> runs one-step-at-a-time, you need to confirm each task before running</li> <li><code>--list-tags</code> lists all available tags</li> <li><code>--list-tasks</code> lists all tasks that would be executed</li> </ul>"},{"location":"ansible/playbook/#with-ansible-navigator","title":"With Ansible Navigator","text":"<p>To ensure that your Ansible Content works when running it locally during development and when running it in AAP or AWX later, it is advisable to execute it with the same Execution Environment. The ansible-playbook command can't run these, this is where the Navigator comes in.  </p> <p>The Ansible (Content) Navigator is a command-line tool and a text-based user interface (TUI) for creating, reviewing, running and troubleshooting Ansible content, including inventories, playbooks, collections, documentation and container images (execution environments). Take a look at the Installation section on how to install the utility and dependencies.</p> <p>Use the following minimal configuration for the Navigator and store it in your project root directory:</p> <p>ansible-navigator.yml</p> <pre><code>---\nansible-navigator:\n  execution-environment:\n    image: ghcr.io/ansible-community/community-ee-base:latest # (1)!\n    pull:\n      policy: missing\n  logging:\n    level: warning\n    file: logs/ansible-navigator.log\n  mode: stdout # (2)!\n  playbook-artifact:\n    enable: true\n    save-as: \"logs/{playbook_status}-{playbook_name}-{time_stamp}.json\" # (3)!\n</code></pre> <ol> <li>Specifies the name of the execution environment image to use, change this, if you want to use your own. The pull policy will download the image if it is not already present (this also means no updated images will be downloaded!). To build and use your own Execution Environment take a look at the section Installation &gt; Execution Environments.</li> <li>Specifies the user-interface mode, with <code>stdout</code> it will output to standard-out as with the usual <code>ansible-playbook</code> command. Use <code>interactive</code> to use the TUI. You can provide the CLI-parameter <code>-m</code> or <code>--mode</code> to overwrite the configuration.</li> <li>Specifies the name for artifacts created from completed playbooks. For example, for a successful run of the <code>site.yml</code> playbook a log file like <code>logs/successful-site-2023-11-01T12:20:20.907856+00:00.json</code>. For failed runs it would be <code>logs/failed-site-2023-11-01T12:29:17.020432+00:00.json</code>. With the replay command, you now can observe output of previous playbook runs, e.g. <code>ansible-navigator replay logs/failed-site-2023-11-01T12\\:29\\:33.129179+00\\:00.json</code>.</li> </ol> <p>You can also use the Navigator configuration for all your projects, save it as a hidden file in your home directory (e.g. <code>~/.ansible-navigator.yml</code>).</p> <p>Take a look at the official Ansible Navigator Documentation for all other configuration options.</p> <p>Warning</p> <p>With the configuration above, playbook artifacts (logs), as well as the Navigator Log-file, will be stored in a <code>logs</code> folder in your playbook directory. Consider ignoring the folder from Git tracking.</p> .gitignore<pre><code>logs/\n</code></pre> <p>Executing a playbook with the Navigator is as easy as before, just run it like this:</p> <pre><code>ansible-navigator run site.yml\n</code></pre> <p>Append any CLI-parameters (e.g. <code>-i inventory.ini</code>) that you are used to as when executing it with ansible-playbook.</p> <p>Tip</p> <p>Using the Interactive mode (the TUI) is encouraged, try around!</p>"},{"location":"ansible/project/","title":"Project","text":""},{"location":"ansible/project/#version-control","title":"Version Control","text":"<p>Keep your playbooks and inventory file in git (or another version control system), and commit when you make changes to them. This way you have an audit trail describing when and why you changed the rules that are automating your infrastructure.</p> <p>Tip</p> <p>Always use version control!</p> <p>Take a look at the Development section for additional information.</p>"},{"location":"ansible/project/#ansible-configuration","title":"Ansible configuration","text":"<p>Always use a project-specific <code>ansible.cfg</code> in the parent directory of your project. The following configuration can be used as a starting point:</p> <pre><code>[defaults]\n# Define inventory, no need to provide '-i' anymore.\ninventory = inventory/production.ini\n\n# Playbook-Output in YAML instead of JSON\ncallback_result_format = yaml\n</code></pre>"},{"location":"ansible/project/#show-check-mode","title":"Show check mode","text":"<p>The following parameter enables displaying markers when running in check mode.</p> <pre><code>[defaults]\ncheck_mode_markers = true\n</code></pre> <p>The markers are <code>DRY RUN</code> at the beginning and ending of playbook execution (when calling <code>ansible-playbook --check</code>) and <code>CHECK MODE</code> as a suffix at every play and task that is run in check mode.</p> Example <pre><code>$ ansible-playbook -i inventory.ini playbook.yml -C\n\nDRY RUN ******************************************************************\n\nPLAY [Install and configure Worker Nodes] [CHECK MODE] *******************\n\nTASK [Gathering Facts] [CHECK MODE] **************************************\nok: [k8s-worker1]\nok: [k8s-worker2]\nok: [k8s-worker2]\n\n...\n</code></pre>"},{"location":"ansible/project/#show-task-path-when-failed","title":"Show task path when failed","text":"<p>For easier development when handling with very big playbooks, it may be useful to know which file holds the failed task. To display the path to the file containing the failed task and the line number, add this parameter:</p> <pre><code>[defaults]\nshow_task_path_on_failure = true\n</code></pre> Example <p>When set to <code>true</code>:</p> <pre><code>...\n\nTASK [Set motd message for k8s worker node] **************************************************\ntask path: /home/timgrt/kubernetes_installation/roles/kube_worker/tasks/configure.yml:39\nfatal: [k8s-worker1]: FAILED! =&gt;\n...\n</code></pre> <p>When set to <code>false</code>:</p> <pre><code>...\n\nTASK [Set motd message for k8s worker node] ****************************************************\nfatal: [k8s-worker1]: FAILED! =&gt;\n...\n</code></pre> <p>Even if you don't set this, the path is displayed automatically for every task when running with <code>-vv</code> or greater verbosity, but you'll need to run the playbook again.</p>"},{"location":"ansible/project/#dependencies","title":"Dependencies","text":"<p>Your project will have certain dependencies, make sure to provide a <code>requirements.yml</code> for necessary Ansible collections and a <code>requirements.txt</code> for necessary Python packages. Consider using Execution Environments where all dependencies are combined in a Container Image.</p>"},{"location":"ansible/project/#collections","title":"Collections","text":"<p>Always provide a <code>requirements.yml</code> with all collections used within your project. This makes sure that required collections can be installed, if only the ansible-core binary is installed.</p> <pre><code>---\ncollections:\n  - community.general\n  - ansible.posix\n\n  - name: cisco.ios\n    version: '&gt;=3.1.0'  \n</code></pre> <p>Install all collections from the requirements-file:</p> <pre><code>ansible-galaxy collection install -r requirements.yml\n</code></pre>"},{"location":"ansible/project/#python-packages","title":"Python packages","text":"<p>Always provide a <code>requirements.txt</code> with all Python packages need by modules used within your project.</p> <pre><code>boto\nopenshift&gt;=0.6\nPyYAML&gt;=3.11\n</code></pre> <p>Install all dependencies from the requirements-file:</p> <pre><code>pip3 install -r requirements.txt\n</code></pre>"},{"location":"ansible/project/#directory-structure","title":"Directory structure","text":"<pre><code>.\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 hosts\n\u251c\u2500\u2500 k8s_install.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 requirements.yml\n\u2514\u2500\u2500 roles\n    \u251c\u2500\u2500 k8s_bootstrap\n    \u2502   \u251c\u2500\u2500 files\n    \u2502   \u2502   \u251c\u2500\u2500 daemon.json\n    \u2502   \u2502   \u2514\u2500\u2500 k8s.conf\n    \u2502   \u251c\u2500\u2500 tasks\n    \u2502   \u2502   \u251c\u2500\u2500 install_kubeadm.yml\n    \u2502   \u2502   \u251c\u2500\u2500 main.yml\n    \u2502   \u2502   \u2514\u2500\u2500 prerequisites.yml\n    \u2502   \u2514\u2500\u2500 templates\n    \u2502       \u2514\u2500\u2500 kubernetes.repo.j2\n    \u251c\u2500\u2500 k8s_control_plane\n    \u2502   \u251c\u2500\u2500 files\n    \u2502   \u2502   \u2514\u2500\u2500 kubeconfig.sh\n    \u2502   \u2514\u2500\u2500 tasks\n    \u2502       \u2514\u2500\u2500 main.yml\n    \u2514\u2500\u2500 k8s_worker_nodes\n        \u2514\u2500\u2500 tasks\n            \u2514\u2500\u2500 main.yml\n</code></pre>"},{"location":"ansible/project/#filenames","title":"Filenames","text":"<p>Folder- and file-names consisting of multiple words are separated with underscores (e.g. <code>roles/grafana_deployment/tasks/grafana_installation.yml</code>). YAML files are saved with the extension <code>.yml</code>.  </p> GoodBad <pre><code>.\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 hosts\n\u251c\u2500\u2500 k8s_install.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.yml\n\u2514\u2500\u2500 roles\n    \u251c\u2500\u2500 k8s_bootstrap\n    \u2502   \u251c\u2500\u2500 files\n    \u2502   \u2502   \u251c\u2500\u2500 daemon.json\n    \u2502   \u2502   \u2514\u2500\u2500 k8s.conf\n    \u2502   \u251c\u2500\u2500 tasks\n    \u2502   \u2502   \u251c\u2500\u2500 install_kubeadm.yml\n    \u2502   \u2502   \u251c\u2500\u2500 main.yml\n    \u2502   \u2502   \u2514\u2500\u2500 prerequisites.yml\n    \u2502   \u2514\u2500\u2500 templates\n    \u2502       \u2514\u2500\u2500 kubernetes.repo.j2\n    \u251c\u2500\u2500 k8s_control_plane\n    \u2502   \u251c\u2500\u2500 files\n    \u2502   \u2502   \u2514\u2500\u2500 kubeconfig.sh\n    \u2502   \u2514\u2500\u2500 tasks\n    \u2502       \u2514\u2500\u2500 main.yml\n    \u2514\u2500\u2500 k8s_worker_nodes\n        \u2514\u2500\u2500 tasks\n            \u2514\u2500\u2500 main.yml\n</code></pre> <p>Playbook-name without underscores and wrong file extension, role folders or task files inconsistent, with underscores and wrong extension. <pre><code>.\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 hosts\n\u251c\u2500\u2500 k8s-install.yaml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 roles\n    \u251c\u2500\u2500 k8s-bootstrap\n    \u2502   \u251c\u2500\u2500 files\n    \u2502   \u2502   \u251c\u2500\u2500 daemon.json\n    \u2502   \u2502   \u2514\u2500\u2500 k8s.conf\n    \u2502   \u251c\u2500\u2500 tasks\n    \u2502   \u2502   \u251c\u2500\u2500 installKubeadm.yaml\n    \u2502   \u2502   \u251c\u2500\u2500 main.yml\n    \u2502   \u2502   \u2514\u2500\u2500 prerequisites.yaml\n    \u2502   \u2514\u2500\u2500 templates\n    \u2502       \u2514\u2500\u2500 kubernetes.repo.j2\n    \u251c\u2500\u2500 k8sControlPlane\n    \u2502   \u251c\u2500\u2500 files\n    \u2502   \u2502   \u2514\u2500\u2500 kubeconfig.sh\n    \u2502   \u2514\u2500\u2500 tasks\n    \u2502       \u2514\u2500\u2500 main.yaml\n    \u2514\u2500\u2500 k8s_worker-nodes\n        \u2514\u2500\u2500 tasks\n            \u2514\u2500\u2500 main.yaml\n</code></pre></p>"},{"location":"ansible/project/#yaml-syntax","title":"YAML Syntax","text":"<p>Following a basic YAML coding style across the whole team improves readability and reusability.</p>"},{"location":"ansible/project/#indentation","title":"Indentation","text":"<p>Two spaces are used to indent everything, e.g. list items or dictionary keys.</p> GoodBad <p>Playbook: <pre><code>---\n- name: Initialize Control-Plane Nodes\n  hosts: kubemaster\n  become: true\n  roles:\n    - k8s_control_plane\n\n- name: Install and configure Worker Nodes\n  hosts: kubeworker\n  become: true\n  roles:\n    - k8s_worker_nodes\n</code></pre> Variable-file: <pre><code>ntp_server_list:\n  - 0.de.pool.ntp.org\n  - 1.de.pool.ntp.org\n  - 2.de.pool.ntp.org\n  - 3.de.pool.ntp.org\n</code></pre></p> <p>Playbook with roles not indented by two whitespaces. <pre><code>- name: Demo play\n  hosts: database_servers\n  roles:\n  - common\n  - postgres\n</code></pre> List in variable-file indented with four whitespaces: <pre><code>ntp_server_list:\n    - 0.de.pool.ntp.org\n    - 1.de.pool.ntp.org\n    - 2.de.pool.ntp.org\n    - 3.de.pool.ntp.org\n</code></pre></p> <p>The so-called YAML \"one-line\" syntax is not used, neither for passing parameters in tasks, nor for lists or dictionaries.</p> GoodBad <p><pre><code>---\n- name: Install Apache from the testing repo\n  ansible.builtin.package:\n    name: httpd\n    enablerepo: testing\n    state: present\n</code></pre> <pre><code>- name: Install a list of packages\n  ansible.builtin.package:\n    name:\n      - nginx\n      - postgresql\n      - postgresql_server\n    state: present\n</code></pre></p> <p>Task with One-line syntax: <pre><code>- name: Install the latest version of Apache from the testing repo\n  package: name=httpd enablerepo=testing state=present\n</code></pre> List in task with One-line syntax: <pre><code>- name: Install a list of packages\n  package:\n    name: ['nginx', 'postgresql', 'postgresql-server']\n    state: present\n</code></pre></p>"},{"location":"ansible/project/#booleans","title":"Booleans","text":"<p>Use <code>true</code> and <code>false</code> for boolean values in playbooks. Do not use the Ansible-specific <code>yes</code> and <code>no</code> as boolean values in YAML as these are completely custom extensions used by Ansible and are not part of the YAML spec. Also, avoid the use of the Python-style <code>True</code> and <code>False</code> for boolean values.</p> GoodBad <pre><code>- name: Start and enable service httpd\n  ansible.builtin.service:\n    name: httpd\n    enabled: true\n    state: started\n</code></pre> <pre><code>- name: Start and enable service httpd\n  ansible.builtin.service:\n    name: httpd\n    enabled: yes\n    state: started\n</code></pre> <p>YAML 1.1 allows all variants whereas YAML 1.2 allows only true/false, you can avoid a massive migration effort for when it becomes the default.</p> <p>Use the <code>| bool</code> filter when using bare variables (expressions consisting of just one variable reference without any operator) in <code>when</code> conditions.</p> GoodBad <p>Using a variable <code>upgrade_allowed</code> with the default value <code>false</code>, task is executed when overwritten with <code>true</code> value. <pre><code>- name: Upgrade all packages, excluding kernel &amp; foo related packages # noqa package-latest\n  ansible.builtin.package:\n    name: \"*\"\n    state: latest\n    exclude: kernel*,foo*\n  when: upgrade_allowed | bool\n</code></pre></p> <pre><code>- name: Upgrade all packages, excluding kernel &amp; foo related packages\n  ansible.builtin.package:\n    name: \"*\"\n    state: latest\n    exclude: kernel*,foo*\n  when: upgrade_allowed\n</code></pre>"},{"location":"ansible/project/#quoting","title":"Quoting","text":"<p>Do not use quotes unless you have to, especially for short module-keyword-like strings like present, absent, etc. When using quotes, use the same type of quotes throughout your playbooks. Always use double quotes (<code>\"</code>), whenever possible.</p>"},{"location":"ansible/project/#comments","title":"Comments","text":"<p>Use loads of comments! Well, the name parameter should describe your task in detail, but if your task uses multiple filters or regex's, comments should be used for further explanation. Commented code is generally to be avoided. Playbooks or task files are not committed, if they contain commented out code.  </p> <p>Bad</p> <p>Why is the second task commented? Is it not necessary anymore? Does it not work as expected? <pre><code>- name: Change port to {{ grafana_port }}\n  community.general.ini_file:\n    path: /etc/grafana/grafana.ini\n    section: server\n    option: http_port\n    value: \"{{ grafana_port }}\"\n  become: true\n  notify: restart grafana\n\n# - name: Change theme to {{ grafana_theme }}\n#   ansible.builtin.lineinfile:\n#     path: /etc/grafana/grafana.ini\n#     regexp: '.*default_theme ='\n#     line: \"default_theme = {{ grafana_theme }}\"\n#   become: yes\n#   notify: restart grafana\n</code></pre></p> <p>Comment commented tasks</p> <p>If you really have to comment the whole task, add a description why, when and by whom it was commented.</p>"},{"location":"ansible/roles/","title":"Roles","text":"<p>New playbook functionality is always added in a role. Roles should only serve a defined purpose that is unambiguous by the role name. The role name should be short and unique. It is separated with hyphens, if it consists of several words.</p>"},{"location":"ansible/roles/#readme","title":"Readme","text":"<p>Every role must have a role-specific <code>README.md</code> describing scope and focus of the role. Use the following example:</p> <pre><code># Role name/title\n\nBrief description of the role, what it does and what not.\n\n## Requirements\n\nTechnical requirements, e.g. necessary packages/rpms, own modules or plugins.\n\n## Role Variables\n\nThe role uses the following variables:\n\n| Variable Name | Type    | Default Value | Description            |\n| ------------- | ------- | ------------- | ---------------------- |\n| example       | Boolean | false         | Brief description      |\n\n## Dependencies\n\nThis role expects to run **after** the following roles:\n* repository\n* networking\n* common\n* software\n\n## Tags\n\nThe role can be executed with the following tags:\n* install\n* configure\n* service\n\n## Example Playbook\n\nUse the role in a playbook like this (after running plays/roles from dependencies section):\n```yaml\n- name: Execute role\n  hosts: example_servers\n  become: true\n  roles:\n    - example_role\n```\n\n## Authors\n\nTim Gr\u00fctzmacher - &lt;tim.gruetzmacher@computacenter.com&gt;\n</code></pre>"},{"location":"ansible/roles/#role-structure","title":"Role structure","text":""},{"location":"ansible/roles/#role-skeleton","title":"Role skeleton","text":"<p>The <code>ansible-galaxy</code> utility can be used to create the role skeleton with the following command:</p> <pre><code>ansible-galaxy role init roles/demo\n</code></pre> <p>This would create the following directory:</p> <pre><code>roles/demo/\n\u251c\u2500\u2500 defaults\n\u2502   \u2514\u2500\u2500 main.yml\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 handlers\n\u2502   \u2514\u2500\u2500 main.yml\n\u251c\u2500\u2500 meta\n\u2502   \u2514\u2500\u2500 main.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 tasks\n\u2502   \u2514\u2500\u2500 main.yml\n\u251c\u2500\u2500 templates\n\u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 inventory\n\u2502   \u2514\u2500\u2500 test.yml\n\u251c\u2500\u2500 .travis.yml\n\u2514\u2500\u2500 vars\n    \u2514\u2500\u2500 main.yml\n</code></pre> <p>At least the folders (and content) <code>tests</code> (a sample inventory and playbook for testing, we will use a different testing method) and <code>vars</code> (variable definitions, not used according to this Best Practice Guide, because we use only group_vars, host_vars and defaults) are not necessary. Also the <code>.travis.yml</code> (a CI/CD solution) definition is not useful.</p> <p>Tip</p> <p>Use a custom role skeleton which is used by <code>ansible-galaxy</code>!</p> <p>Consider the following role skeleton, note the missing vars and test folder and the newly added Molecule folder.</p> <pre><code>roles/role_skeleton/\n\u251c\u2500\u2500 defaults\n\u2502   \u2514\u2500\u2500 main.yml\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 handlers\n\u2502   \u2514\u2500\u2500 main.yml\n\u251c\u2500\u2500 meta\n\u2502   \u2514\u2500\u2500 main.yml\n\u251c\u2500\u2500 molecule\n\u2502   \u2514\u2500\u2500 default\n\u2502       \u251c\u2500\u2500 converge.yml\n\u2502       \u2514\u2500\u2500 molecule.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 tasks\n\u2502   \u2514\u2500\u2500 main.yml\n\u2514\u2500\u2500 templates\n</code></pre> <p>You need to define the following parameter in your custom <code>ansible.cfg</code>:</p> <pre><code>[galaxy]\nrole_skeleton = roles/role_skeleton\n</code></pre> <p>Success</p> <p>Afterwards, initializing a new role with <code>ansible-galaxy role init</code> creates a role structure with exactly the content you need!</p>"},{"location":"ansible/tasks/","title":"Tasks","text":"<p>Tasks should always be inside of a role. Do not use tasks in a play directly. Logically related tasks are to be separated into individual files, the <code>main.yml</code> of a role only imports other task files.</p> <pre><code>.\n\u2514\u2500\u2500 roles\n    \u2514\u2500\u2500 k8s_bootstrap\n        \u2514\u2500\u2500 tasks\n            \u251c\u2500\u2500 install_kubeadm.yml\n            \u251c\u2500\u2500 main.yml\n            \u2514\u2500\u2500 prerequisites.yml\n</code></pre> <p>The file name of a task file should describe the content.</p> roles/k8s_bootstrap/tasks/main.yml<pre><code>---\n- ansible.builtin.import_tasks: prerequisites.yml # noqa name[missing]\n- ansible.builtin.import_tasks: install_kubeadm.yml # noqa name[missing]\n</code></pre> <code>noqa</code> statement <p>The file <code>main.yml</code> only references other task-files, still, the ansible-lint utility would trigger, as every task should have the <code>name</code> parameter. While this is correct (and you should always name your actual tasks), the name parameter on import statements is not shown anyway, as they are pre-processed at the time playbooks are parsed. Take a look at the following section regarding import vs. include.  </p> <p>Success</p> <p>Therefore, silencing the linter in this particular case with the <code>noqa</code> statement is acceptable.  </p> <p>In contrast, include statements like <code>ansible.builtin.include_tasks</code> should have the <code>name</code> parameter, as these statements are processed when they are encountered during the execution of the playbook.</p>"},{"location":"ansible/tasks/#import-vs-include","title":"import vs. include","text":"<p>Ansible offers two ways to reuse tasks: statically with <code>ansible.builtin.import_tasks</code> and dynamically with <code>ansible.builtin.include_tasks</code>. Each approach to reuse distributed Ansible artifacts has advantages and limitations, take a look at the Ansible documentation for an in-depth comparison of the two statements.  </p> <p>Tip</p> <p>In most cases, use the static <code>ansible.builtin.import_tasks</code> statement, it has more advantages than disadvantages.</p> <p>One of the biggest disadvantages of the dynamic include_tasks statement, syntax errors are not found easily with <code>--syntax-check</code> or by using ansible-lint. You may end up with a failed playbook, although all your testing looked fine. Take a look at the following example, the recommended <code>ansible.builtin.import_tasks</code> statement on the left, the <code>ansible.builtin.include_tasks</code> statement on the right.</p> <p>Syntax or linting errors found</p> <p>Using static <code>ansible.builtin.import_tasks</code>:</p> roles/prerequisites/tasks/main.yml<pre><code>---\n- ansible.builtin.import_tasks: prerequisites.yml\n- ansible.builtin.import_tasks: install_kubeadm.yml\n</code></pre> <p>Task-file with syntax error (module-parameters are not indented correctly):</p> install_kubeadm.yml<pre><code>- name: Install Kubernetes Repository\n  ansible.builtin.template:\n  src: kubernetes.repo.j2\n  dest: /etc/yum.repos.d/kubernetes.repo\n</code></pre> <p>Running playbook with <code>--syntax-check</code> or running <code>ansible-lint</code>:</p> <pre><code>$ ansible-playbook k8s_install.yml --syntax-check\nERROR! conflicting action statements: ansible.builtin.template, src\n\nThe error appears to be in '/home/timgrt/kubernetes_installation/roles/k8s-bootstrap/tasks/install_kubeadm.yml': line 3, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- name: Install Kubernetes Repository\n  ^ here\n$ ansible-lint k8s_install.yml\nWARNING  Listing 1 violation(s) that are fatal\nsyntax-check[specific]: conflicting action statements: ansible.builtin.template, src\nroles/k8s_bootstrap/tasks/install_kubeadm.yml:3:3\n\n\n                  Rule Violation Summary  \ncount tag                    profile rule associated tags\n    1 syntax-check[specific] min     core, unskippable  \n\nFailed: 1 failure(s), 0 warning(s) on 12 files.\n</code></pre> <p>Syntax or linting errors NOT found!</p> <p>Using dynamic <code>ansible.builtin.include_tasks</code>:</p> roles/prerequisites/tasks/main.yml<pre><code>---\n- ansible.builtin.include_tasks: prerequisites.yml\n- ansible.builtin.include_tasks: install_kubeadm.yml\n</code></pre> <p>Task-file with syntax error (module-parameters are not indented correctly):</p> <p>install_kubeadm.yml<pre><code>- name: Install Kubernetes Repository\n  ansible.builtin.template:\n  src: kubernetes.repo.j2\n  dest: /etc/yum.repos.d/kubernetes.repo\n</code></pre> Running playbook with <code>--syntax-check</code> or running <code>ansible-lint</code>:</p> <pre><code>$ ansible-playbook k8s_install.yml --syntax-check\n\nplaybook: k8s_install.yml\n$ ansible-lint k8s_install.yml\n\nPassed: 0 failure(s), 0 warning(s) on 12 files. Last profile that met the validation criteria was 'production'.\n</code></pre> <p>Danger</p> <p>As the <code>--syntax-check</code> or <code>ansible-lint</code> are doing a static code analysis and the task-files are not included statically, possible syntax errors are not recognized!</p> <p>Your playbook will fail when running it live, revealing the syntax error.</p> <p>Info</p> <p>There are also big differences in resource consumption and performance, imports are quite lean and fast, while includes require a lot of management and accounting.</p>"},{"location":"ansible/tasks/#naming-tasks","title":"Naming tasks","text":"<p>It is possible to leave off the name for a given task, though it is recommended to provide a description about why something is being done instead. This description is shown when the playbook is run. Write task names in the imperative (e.g. \"Ensure service is running\"), this communicates the action of the task. Start with a capital letter.</p> GoodBad <pre><code>- name: Install webserver package\n  ansible.builtin.package:\n    name: httpd\n    state: present\n</code></pre> <p><pre><code>- package:\n    name: httpd\n    state: present\n</code></pre> Using name parameter, but not starting with capital letter, nor describing the task properly. <pre><code>- name: install package\n  package:\n    name: httpd\n    state: present\n</code></pre></p>"},{"location":"ansible/tasks/#tags","title":"Tags","text":"<p>Don't use too many tags, it gets confusing very quickly. Tags should only be allowed for imported task files within the <code>main.yml</code> of a role. Tags at the task level in sub-task files should be avoided.</p> tasks/main.yml<pre><code>---\n- ansible.builtin.import_tasks: installation.yml # noqa name[missing]\n  tags:\n    - install\n\n- ansible.builtin.import_tasks: configuration.yml # noqa name[missing]\n  tags:\n    - configure\n</code></pre> <p>Try to use the same tags across your roles, this way you would be able to run only e.g. installation tasks from multiple roles.</p>"},{"location":"ansible/tasks/#idempotence","title":"Idempotence","text":"<p>Each task must be idempotent, if non-idempotent modules are used (command, shell, raw) these tasks must be developed via appropriate parameters or conditions to an idempotent mode of operation.  </p> <p>Tip</p> <p>In general, the use of non-idempotent modules should be reduced to a necessary minimum.</p>"},{"location":"ansible/tasks/#command-vs-shell-module","title":"command vs. shell module","text":"<p>In most of the use cases, both shell and command modules perform the same job. However, there are few main differences between these two modules. The command module uses the Python interpreter on the target node (as all other modules), the shell module runs a real shell on the target (pipes and redirects are available, as well as access to environment variables).</p> <p>Tip</p> <p>Always try to use the <code>command</code> module over the <code>shell</code> module, if you do not explicitly need shell functionality.</p> <p>Parsing shell meta-characters can lead to unexpected commands being executed if quoting is not done correctly so it is more secure to use the command module when possible. To sanitize any variables passed to the shell module, you should use <code>{{ var | quote }}</code> instead of just <code>{{ var }}</code> to make sure they do not include evil things like semicolons.</p>"},{"location":"ansible/tasks/#creates-and-removes","title":"creates and removes","text":"<p>Check mode is supported for non-idempotent modules when passing <code>creates</code> or <code>removes</code>. If running in check mode and either of these are specified, the module will check for the existence of the file and report the correct changed status. If these are not supplied, the task will be skipped.</p> <p>Warning</p> <p>Work in Progress - More description necessary.</p>"},{"location":"ansible/tasks/#failed_when-and-changed_when","title":"failed_when and changed_when","text":"<p>Warning</p> <p>Work in Progress - More description necessary.</p> GoodBad <pre><code>- name: Install webserver package\n  ansible.builtin.package:\n    name: httpd\n    state: present\n</code></pre> <p>This task never reports a changed state or fails when an error occurs. <pre><code>- name: Install webserver package\n  shell: sudo yum install http\n  changed_when: false\n  failed_when: false\n</code></pre></p>"},{"location":"ansible/tasks/#modules-and-collections","title":"Modules (and Collections)","text":"<p>Use the full qualified collection names (FQCN) for modules, they are supported since Version 2.9 and ensures your tasks are set for the future.</p> GoodBad <pre><code>- name: Install webserver package\n  ansible.builtin.package:\n    name: httpd\n    state: present\n</code></pre> <pre><code>- package:\n    name: httpd\n    state: present\n</code></pre> <p>In Ansible 2.10, many plugins and modules have migrated to Collections on Ansible Galaxy. Your playbooks should continue to work without any changes. Using the FQCN in your playbooks ensures the explicit and authoritative indicator of which collection to use as some collections may contain duplicate module names.</p>"},{"location":"ansible/tasks/#module-parameters","title":"Module parameters","text":""},{"location":"ansible/tasks/#module-defaults","title":"Module defaults","text":"<p>The <code>module_defaults</code> keyword can be used at the play, block, and task level. Any module arguments explicitly specified in a task will override any established default for that module argument. It makes the most sense to define the module defaults at play level, take a look in that section for an example and things to consider.</p>"},{"location":"ansible/tasks/#permissions","title":"Permissions","text":"<p>When using modules like <code>copy</code> or <code>template</code> you can (and should) set permissions for the files/templates deployed with the <code>mode</code> parameter.</p> <p>For those used to /usr/bin/chmod, remember that modes are actually octal numbers. Add a leading zero (or <code>1</code> for setting sticky bit), showing Ansible\u2019s YAML parser it is an octal number and quote it (like <code>\"0644\"</code> or <code>\"1777\"</code>), this way Ansible receives a string and can do its own conversion from string into number.</p> <p>Warning</p> <p>Giving Ansible a number without following one of these rules will end up with a decimal number which can have unexpected results.</p> GoodBad <pre><code>- name: Copy index.html template\n  ansible.builtin.template:\n    src: welcome.html\n    dest: /var/www/html/index.html\n    mode: \"0644\"\n    owner: apache\n    group: apache\n  become: true\n</code></pre> <p>Missing leading zero: <pre><code>- name: copy index\n  template:\n    src: welcome.html\n    dest: /var/www/html/index.html\n    mode: 644\n    owner: apache\n    group: apache\n  become: true\n</code></pre> This leads to these permissions! <pre><code>[root@demo /]# ll /var/www/html/\ntotal 68\n--w----r-T 1 apache apache 67691 Nov 18 14:30 index.html\n</code></pre></p>"},{"location":"ansible/tasks/#state-definition","title":"State definition","text":"<p>The <code>state</code> parameter is optional to a lot of modules. Whether <code>state: present</code> or <code>state: absent</code>, it\u2019s always best to leave that parameter in your playbooks to make it clear, especially as some modules support additional states.</p>"},{"location":"ansible/tasks/#files-vs-templates","title":"Files vs. Templates","text":"<p>Ansible differentiates between files for static content (deployed with <code>copy</code> module) and templates for content, which should be rendered dynamically with Jinja2 (deployed with <code>template</code> module).  </p> <p>Tip</p> <p>In almost every case, use templates, deployed via <code>template</code> module.  </p> <p>Even if there currently is nothing in the file that is being templated, if there is the possibility in the future that it might be added, having the file handled by the <code>template</code> module makes adding that functionality much simpler than if the file is initially handled by the <code>copy</code> module( and then needs to be moved before it can be edited).</p> <p>Additionally, you now can add a marker, indicating that manual changes to the file will be lost:</p> TemplateRendered output <pre><code>{{ ansible_managed | ansible.builtin.comment }}\n</code></pre> <pre><code>#\n# Ansible Managed\n#\n</code></pre> <code>ansible.builtin.comment</code> filter <p>By default, <code>{{ ansible_managed }}</code> is replaced by the string <code>Ansible Managed</code> as is (can be adjusted in the <code>ansible.cfg</code>). In most cases, the appropriate comment symbol must be prefixed, this should be done with the <code>ansible.builtin.comment</code> filter. For example, <code>.xml</code> files need to be commented differently, which can be configured:</p> TemplateRendered output <pre><code>{{ ansible_managed | ansible.builtin.comment('xml') }}\n</code></pre> <pre><code>&lt;!--\n-\n- Ansible managed\n-\n--&gt;\n</code></pre> <p>You can also use the <code>decorate</code> parameter to choose the symbol yourself. Take a look at the Ansible documentation for additional information.</p> <p>When using the <code>template</code> module, append <code>.j2</code> to the template file name. Keep filenames and templates as close to the name on the destination system as possible.</p>"},{"location":"ansible/tasks/#conditionals","title":"Conditionals","text":"<p>If the <code>when:</code> condition results in a line that is very long, and is an <code>and</code> expression, then break it into a list of conditions.</p> GoodBad <pre><code>- name: Set motd message for k8s worker node\n  ansible.builtin.copy:\n    content: \"This host is used as k8s worker.\\n\"\n    dest: /etc/motd\n    mode: \"0644\"\n  when:\n    - inventory_hostname in groups['kubeworker']\n    - kubeadm_join_result.rc == 0\n</code></pre> <pre><code>- name: Set motd message for k8s worker node\n  copy:\n    content: \"This host is used as k8s worker.\\n\"\n    dest: /etc/motd\n  when: inventory_hostname in groups['kubeworker'] and kubeadm_join_result.rc == 0\n</code></pre> <p>When using conditions on blocks, move the <code>when</code> statement to the top, below the name parameter, to improve readability.</p> GoodBad <pre><code>- name: Install, configure, and start Apache\n  when: ansible_facts['distribution'] == 'CentOS'\n  block:\n    - name: Install httpd and memcached\n      ansible.builtin.package:\n        name:\n          - httpd\n          - memcached\n        state: present\n\n    - name: Apply the foo config template\n      ansible.builtin.template:\n        src: templates/src.j2\n        dest: /etc/foo.conf\n        mode: \"0644\"\n\n    - name: Start service bar and enable it\n      ansible.builtin.service:\n        name: bar\n        state: started\n        enabled: true\n</code></pre> <pre><code>- name: Install, configure, and start Apache\n  block:\n    - name: Install httpd and memcached\n      ansible.builtin.package:\n        name:\n        - httpd\n        - memcached\n        state: present\n\n    - name: Apply the foo config template\n      ansible.builtin.template:\n        src: templates/src.j2\n        dest: /etc/foo.conf\n\n    - name: Start service bar and enable it\n      ansible.builtin.service:\n        name: bar\n        state: started\n        enabled: True\n  when: ansible_facts['distribution'] == 'CentOS'\n</code></pre> <p>Avoid the use of <code>when: foo_result is changed</code> whenever possible. Use handlers, and, if necessary, handler chains to achieve this same result.</p>"},{"location":"ansible/tasks/#loops","title":"Loops","text":"<p>Warning</p> <p>Work in Progress - More description necessary.</p> <p>Converting from <code>with_&lt;lookup&gt;</code> to <code>loop</code> is described with a Migration Guide in the Ansible documentation</p>"},{"location":"ansible/tasks/#limit-loop-output","title":"Limit loop output","text":"<p>When looping over complex data structures, the console output of your task can be enormous. To limit the displayed output, use the <code>label</code> directive with <code>loop_control</code>. For example, this tasks creates users with multiple parameters in a loop:</p> <pre><code>- name: Create local users\n  ansible.builtin.user:\n    name: \"{{ item.name }}\"\n    groups: \"{{ item.groups }}\"\n    append: \"{{ item.append }}\"\n    comment: \"{{ item.comment }}\"\n    generate_ssh_key: true\n    password_expire_max: \"{{ item.password_expire_max }}\"\n  loop: \"{{ user_list }}\"\n  loop_control:\n    label: \"{{ item.name }}\" # (1)!\n</code></pre> <ol> <li> <p>Content of variable <code>user_list</code>:</p> <pre><code>user_list:\n  - name: tgruetz\n    groups: admins,docker\n    append: false\n    comment: Tim Gr\u00fctzmacher\n    shell: /bin/bash\n    password_expire_max: 180\n  - name: joschmi\n    groups: developers,docker\n    append: true\n    comment: Jonathan Schmidt\n    shell: /bin/zsh\n    password_expire_max: 90\n  - name: mfrink\n    groups: developers\n    append: true\n    comment: Mathias Frink\n    shell: /bin/bash\n    password_expire_max: 90\n</code></pre> </li> </ol> <p>Running the playbook results in the following task output, only the content of the name parameter is shown instead of all key-value pairs in the list item.</p> GoodBad <pre><code>TASK [common : Create local users] *********************************************\nFriday 18 November 2022  12:18:01 +0100 (0:00:01.955)       0:00:03.933 *******\nchanged: [demo] =&gt; (item=tgruetz)\nchanged: [demo] =&gt; (item=joschmi)\nchanged: [demo] =&gt; (item=mfrink)\n</code></pre> <p>Not using the <code>label</code> in the <code>loop_control</code> dictionary results in a very long output: <pre><code>TASK [common : Create local users] *********************************************\nFriday 18 November 2022  12:22:40 +0100 (0:00:01.512)       0:00:03.609 *******\nchanged: [demo] =&gt; (item={'name': 'tgruetz', 'groups': 'admins,docker', 'append': False, 'comment': 'Tim Gr\u00fctzmacher', 'shell': '/bin/bash', 'password_expire_max': 90})\nchanged: [demo] =&gt; (item={'name': 'joschmi', 'groups': 'developers,docker', 'append': True, 'comment': 'Jonathan Schmidt', 'shell': '/bin/zsh', 'password_expire_max': 90})\nchanged: [demo] =&gt; (item={'name': 'mfrink', 'groups': 'developers', 'append': True, 'comment': 'Mathias Frink', 'shell': '/bin/bash', 'password_expire_max': 90})\n</code></pre></p>"},{"location":"ansible/tasks/#filter","title":"Filter","text":"<p>Warning</p> <p>Work in Progress - More description necessary.</p>"},{"location":"ansible/variables/","title":"Variables","text":""},{"location":"ansible/variables/#where-to-put-variables","title":"Where to put variables","text":"<p>I always store all my variables at the following three locations:</p> <ul> <li>group_vars folder</li> <li>host_vars folder</li> <li>defaults folder in roles</li> </ul> <p>The defaults-folder contains only default values for all variables used by the role.</p>"},{"location":"ansible/variables/#naming-variables","title":"Naming Variables","text":"<p>The variable name should be self-explanatory (as brief as possible, as detailed as necessary), use multiple words and don't shorten things.</p> <ul> <li>Multiple words are separated with underscores (<code>_</code>)</li> <li>List-Variables are suffixed with <code>_list</code></li> <li>Dictionary-Variables are suffixed with <code>_dict</code></li> <li>Boolean values are provided with lowercase <code>true</code> or <code>false</code></li> </ul> GoodBad <pre><code>download_directory: ~/.local/bin\ncreate_key: true\nneeds_agent: false\n</code></pre> <pre><code>dir: ~/.local/bin\ncreate_key: yes\nneedsAgent: no\nknows_oop: True\n</code></pre>"},{"location":"ansible/variables/#referencing-variables","title":"Referencing variables","text":"<p>After a variable is defined, use Jinja2 syntax to reference it. Jinja2 variables use double curly braces (<code>{{</code> and <code>}}</code>). Use spaces after and before the double curly braces and the variable name. When referencing list or dictionary variables, try to use the bracket notation instead of the dot notation. Bracket notation always works and you can use variables inside the brackets. Dot notation can cause problems because some keys collide with attributes and methods of python dictionaries.</p> GoodBad <p>Simple variable reference: <pre><code>- name: Deploy configuration file\n  ansible.builtin.template:\n    src: foo.cfg.j2\n    dest: \"{{ remote_install_path }}/foo.cfg\"\n    mode: \"0644\"\n</code></pre> Bracket-notation and using variable (interface_name) inside: <pre><code>- name: Output IPv4 address of interface {{ interface_name }}\n  ansible.builtin.debug:\n    msg: \"{{ ansible_facts[interface_name]['ipv4']['address'] }}\"\n</code></pre></p> <p>Not using whitespaces around variable name. <pre><code>- name: Deploy configuration file\n  ansible.builtin.template:\n    src: foo.cfg.j2\n    dest: \"{{remote_install_path}}/foo.cfg\"\n</code></pre> Not using whitespaces and using dot-notation. <pre><code>- name: Output IPv4 address of eth0 interface\n  ansible.builtin.debug:\n    msg: \"{{ansible_facts.eth0.ipv4.address}}\"\n</code></pre></p>"},{"location":"ansible/variables/#encrypted-variables","title":"Encrypted variables","text":"<p>Tip</p> <p>All variables with sensitive content should be vault-encrypted.  </p> <p>Although encrypting just the value of a single variable is possible (with <code>ansible-vault encrypt_string</code>), you should avoid this. Store all sensitive variables in a single file and encrypt the whole file. For example, to store sensitive variables in <code>group_vars</code>, create the subdirectory for the group and within create two files named <code>vars.yml</code> and <code>vault.yml</code>. Inside of the <code>vars.yml</code> file, define all of the variables needed, including any sensitive ones. Next, copy all of the sensitive variables over to the <code>vault.yml</code> file and prefix these variables with <code>vault_</code>. Adjust the variables in the vars file to point to the matching vault_ variables using Jinja2 syntax, and ensure that the vault file is vault encrypted.</p> GoodBad <p><pre><code>---\n# file: group_vars/database_servers/vars.yml\nusername: \"{{ vault_username }}\"\npassword: \"{{ vault_password }}\"\n</code></pre> <pre><code>---\n# file: group_vars/database_servers/vault.yml\n# NOTE: THIS FILE MUST ALWAYS BE VAULT-ENCRYPTED\nvault_username: admin\nvault_password: ex4mple\n</code></pre></p> I can still read the credentials...? <p>Obviously, you wouldn't be able to read the content of the file <code>group_vars/database_servers/vault.yml</code>, as the file would be encrypted. This only demonstrates how the variables are referencing each other. The encrypted <code>vault.yml</code> file looks something like this: <pre><code>$ANSIBLE_VAULT;1.1;AES256\n30653164396132376333316665656131666165613863343330616666376264353830323234623631\n6361303062336532303665643765336464656164363662370a663834313837303437323332336631\n65656335643031393065333366366639653330353634303664653135653230656461666266356530\n3935346533343834650a323934346666383032636562613966633136663631636435333834393261\n36363833373439333735653262306331333062383630623432633134386138656636343137333439\n61633965323066633433373137383330366466366332626334633234376231393330363335353436\n62383866616232323132376366326161386561666238623731323835633237373036636561666165\n36363838313737656232376365346136633934373861326130636531616438643036656137373762\n39616234353135613063393536306536303065653231306166306432623232356465613063336439\n34636232346334386464313935356537323832666436393336366536626463326631653137313639\n36353532623161653266666436646135396632656133623762643131323439613534643430636333\n31386635613238613233\n</code></pre></p> <pre><code># file: group_vars/database_servers.yml\nusername: admin\npassword: ex4mple\n</code></pre> <p>Defining variables this way makes sure that you can still find them with grep. Encrypting files can be done with this command:</p> <pre><code>ansible-vault encrypt group_vars/database_servers/vault.yml\n</code></pre> <p>Once a variable file is encrypted, it should not be decrypted again (because it may get committed unencrypted). View or edit the file like this:</p> <pre><code>ansible-vault view group_vars/database_servers/vault.yml\n</code></pre> <pre><code>ansible-vault edit group_vars/database_servers/vault.yml\n</code></pre> <p>Warning</p> <p>There are modules which will print the values of encrypted variables into STDOUT while using them or with higher verbosity. Be sure to check the parameters and return values of all modules which use encrypted variables!</p> <p>A good example is the <code>ansible.builtin.user</code> module, it automatically obfuscates the value for the password parameter, replacing it with the string <code>NOT_LOGGING_PASSWORD</code>. The <code>ansible.builtin.debug</code> module on the other hand is a bad example, it will output the password in clear-text (well, by design, but this is not what you would expect)!</p> <p>Success</p> <p>Always add the <code>no_log: true</code> key-value-pair for tasks that run the risk of leaking vault-encrypted content!</p> GoodBad <pre><code>---\n- name: Using no_log parameter\n  hosts: database_servers\n  tasks:\n    - name: Add user\n      ansible.builtin.user:\n        name: \"{{ username }}\"\n        password: \"{{ password }}\"\n\n    - name: Debugging a vaulted variable with no_log\n      ansible.builtin.debug:\n        msg: \"{{ password }}\"\n      no_log: true\n</code></pre> Output of playbook run <p>Using the stdout_callback: community.general.yaml for better readability, see Ansible configuration for more info. <pre><code>$ ansible-playbook nolog.yml -v\n\n[...]\n\nTASK [Add user] *********************************************\n[WARNING]: The input password appears not to have been hashed. The 'password'\nargument must be encrypted for this module to work properly.\nok: [db_server1] =&gt; changed=false\n  append: false\n  comment: ''\n  group: 1002\n  home: /home/admin\n  move_home: false\n  name: admin\n  password: NOT_LOGGING_PASSWORD\n  shell: /bin/bash\n  state: present\n  uid: 1002\n\nASK [Debugging a vaulted Variable with no_log] *************\nok: [db_server1] =&gt;\n  censored: 'the output has been hidden due to the fact that ''no_log: true'' was specified for this result'\n\n[...]\n</code></pre> The debug task does not print the value of the password, the output is censored.</p> <p>Hint</p> <p>Observing the output from the \"Add user\" task, you can see that the value of the password parameter is not shown. The warning from the \"Add user\" task stating an unencrypted password is related to not having hashed the password. You can achieve this by using the password_hash filter: <pre><code>password: \"{{ vault_password | password_hash('sha512', 'mysecretsalt') }}\"\n</code></pre> This example uses the string <code>mysecretsalt</code> for salting, in cryptography, a salt is random data that is used as an additional input to a one-way function. Consider using a variable for the salt and treat it the same as the password itself! <pre><code>password: \"{{ vault_password | password_hash('sha512', vault_salt) }}\"\n</code></pre> In this example, the salt is stored in a variable, the same way as the password itself. If you hashed the password, the warning will disappear.</p> <pre><code>- name: Not using no_log parameter\n  hosts: database_servers\n  become: true\n  tasks:\n    - name: Add user\n      ansible.builtin.user:\n        name: \"{{ username }}\"\n        password: \"{{ password }}\"\n\n    - name: Debugging a vaulted Variable\n      ansible.builtin.debug:\n        msg: \"{{ password }}\"\n\u200b\n</code></pre> Output of playbook run <pre><code>$ ansible-playbook nolog.yml -v\n\n[...]\n\nTASK [Add user] *********************************************\n[WARNING]: The input password appears not to have been hashed. The 'password'\nargument must be encrypted for this module to work properly.\nok: [db_server1] =&gt; changed=false\n  append: false\n  comment: ''\n  group: 1002\n  home: /home/admin\n  move_home: false\n  name: admin\n  password: NOT_LOGGING_PASSWORD\n  shell: /bin/bash\n  state: present\n  uid: 1002\n\nASK [Debugging a vaulted Variable with no_log] *************\nok: [db_server1] =&gt;\n  msg: ex4mple\n\n[...]\n</code></pre>"},{"location":"ansible/variables/#prevent-unintentional-commits","title":"Prevent unintentional commits","text":"<p>Use a pre-commit hook to prevent accidentally committing unencrypted sensitive content. The easiest way would be to use the pre-commit framework/tool with the following configuration:</p> .pre-commit-config.yaml<pre><code>repos:\n  - repo: https://github.com/timgrt/pre-commit-hooks\n      rev: v0.2.1\n      hooks:\n        - id: check-vault-files\n</code></pre> <p>Take a look at the development section for additional information.</p>"},{"location":"ansible/variables/#variable-validation","title":"Variable validation","text":"<p>Playbooks often need user input, this may lead to errors like</p> <ul> <li>required variables not provided</li> <li>wrong variable type (e.g. integer instead of string, string instead of list, ...)</li> <li>typos in variable values</li> <li>...</li> </ul> <p>It is useful to validate the user input early and provide a meaningful error message, if necessary.</p>"},{"location":"ansible/variables/#assert-module","title":"Assert module","text":"<p>For simple variable validations, use the <code>ansible.builtin.assert</code> module, it checks if a given expressions evaluates to true.</p> <pre><code>- name: Ensure AAP credentials are provided\n  ansible.builtin.assert:\n    that:\n      - lookup('env', 'CONTROLLER_HOST') | length &gt; 0\n      - lookup('env', 'CONTROLLER_HOST') | length &gt; 0\n      - lookup('env', 'CONTROLLER_HOST') | length &gt; 0\n    quiet: true\n    fail_msg: |\n      AAP login credentials are missing!\n      Export environment variables locally ir add the correct credential to the Job template.\n</code></pre> <p>The task above for example checks if three environment variables are set or rather contain input (the lookup plugin produces an empty string if the environment variable is not found). If the environment variable is found and is longer than zero, the expressions evaluates to true, otherwise the error message in the <code>fail_msg</code> parameter is shown and the playbook fails (for this host).</p>"},{"location":"ansible/variables/#validate-module","title":"Validate module","text":"<p>Input validation for complex (deeply nested) variables can be challenging with the <code>ansible.builtin.assert</code> module, therefore use the <code>ansible.utils.validate</code> module. By default, the JSON Schema engine is used by the module to validate the data with the provided criteria, other engines can be used as well.  </p> <p>JSON Schema is extremely widely used and nearly equally widely implemented. There are implementations of JSON Schema validation for many programming languages or environments (e.g. you can use it in VScode where it will validate your variable files, while you are writing it, before you even run the playbook.) and it is well documented. It provides</p> <ul> <li>Structured Data Description</li> <li>Rule Definition and Enforcement</li> <li>Produce clear documentation</li> <li>Extensibility</li> <li>Data Validation</li> </ul> <p>Take a look at the following example.</p> Variable FileJSON Schema <p><pre><code>---\nserver_list:\n  - fqdn: server1.example.com\n    ipv4_address_list:\n      - 10.0.5.36\n      - 192.168.2.67\n    cores: 4\n    memory: 16GB\n    disk_space: 100GB\n    business_owner: john.doe@example.com # (1)!\n  - fqdn: server2 # (2)!\n    ipv4_address_list:\n      - 10.0.5.55\n      - 192.168.2.89\n    cores: 2\n    memory: 8 # (3)!\n    disk_space: 100GB\n</code></pre></p> <ol> <li>This is an optional field, as you can in the JSON Schema definition in line 47</li> <li>That is not a FQDN! The JSON Schema definition validates that it is (line 17), as well as checking if the required domain is used.</li> <li>The memory value is expected to be a string, prefixed with GB. The JSON Schema definition (line 34) ensures the correct type and prefix.</li> </ol> <pre><code>{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"title\": \"Server List variable file validation\",\n    \"description\": \"A schema to validate the variable file containing the server list\",\n    \"type\": \"object\",\n    \"additionalProperties\": false,\n    \"properties\": {\n        \"server_list\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"additionalProperties\": false,\n                \"properties\": {\n                    \"fqdn\": {\n                        \"type\": \"string\",\n                        \"description\": \"Server name with example.com domain\",\n                        \"pattern\": \"^([a-z0-9]+)(\\\\.example\\\\.com)$\"\n                    },\n                    \"ipv4_address_list\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"string\",\n                            \"format\": \"ipv4\"\n                        }\n                    },\n                    \"cores\": {\n                        \"type\": \"number\",\n                        \"minimum\": 1,\n                        \"maximum\": 64\n                    },\n                    \"memory\": {\n                        \"type\": \"string\",\n                        \"description\": \"Memory size in GB, expects a number followed by GB\",\n                        \"pattern\": \"^\\\\d+GB$\"\n                    },\n                    \"disk_space\": {\n                        \"type\": \"string\",\n                        \"description\": \"Disk size in GB, expects a number followed by GB\",\n                        \"pattern\": \"^\\\\d+GB$\"\n                    },\n                    \"business_owner\": {\n                        \"type\": \"string\",\n                        \"description\": \"Email address of the responsible person for this server\",\n                        \"format\": \"email\"\n                    }\n                },\n                \"required\": [\n                    \"fqdn\",\n                    \"ipv4_address_list\",\n                    \"cores\",\n                    \"memory\",\n                    \"disk_space\"\n                ]\n            }\n        }\n    },\n    \"required\": [\n        \"server_list\"\n    ]\n}\n</code></pre> <p>To validate the variable file with the provided JSON Schema file, use the following task:</p> <pre><code>- name: Variable file validation\n  ansible.utils.validate:\n    data: \"{{ lookup('file', 'variables.yml') | from_yaml | to_json }}\"\n    criteria: \"{{ lookup('file', 'json_schemas/server_list_validation.json') }}\"\n    engine: ansible.utils.jsonschema\n</code></pre> <p>The files are read in with a file lookup, the variables file is converted to JSON. To be able to use the module (or the filter-, test- or lookup-Plugin with the same name), you'll need the <code>ansible.utils</code> collection and an additional Python package:</p> <pre><code>ansible-galaxy collection install ansible.utils\n</code></pre> <pre><code>pip3 install jsonschema\n</code></pre> Playbook output showing validation errors <pre><code>TASK [Variable file validation] ****************************************************************************************\nfatal: [localhost]: FAILED! =&gt;\n    changed: false\n    errors:\n    -   data_path: server_list.1.fqdn\n        expected: ^([a-z0-9]+)(\\.example\\.com)$\n        found: server2\n        json_path: $.server_list[1].fqdn\n        message: '''server2'' does not match ''^([a-z0-9]+)(\\\\.example\\\\.com)$'''\n        relative_schema:\n            description: Server name with example.com domain\n            pattern: ^([a-z0-9]+)(\\.example\\.com)$\n            type: string\n        schema_path: properties.server_list.items.properties.fqdn.pattern\n        validator: pattern\n    -   data_path: server_list.1.memory\n        expected: string\n        found: 8\n        json_path: $.server_list[1].memory\n        message: 8 is not of type 'string'\n        relative_schema:\n            description: Memory size in GB, expects a number followed by GB\n            pattern: ^\\d+GB$\n            type: string\n        schema_path: properties.server_list.items.properties.memory.type\n        validator: type\n    msg: |-\n        Validation errors were found.\n        At 'properties.server_list.items.properties.fqdn.pattern' 'server2' does not match '^([a-z0-9]+)(\\\\.example\\\\.com)$'.\n        At 'properties.server_list.items.properties.memory.type' 8 is not of type 'string'.\n\nPLAY RECAP *************************************************************************************************************\nlocalhost                  : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0  \n</code></pre> <p>To create the initial JSON schema for your variable input, multiple tools are available online, for example to convert from YAML to JSON and afterwards from JSON to JSON-Schema.</p> Usage with <code>ansible.utils.validate</code> filter plugin, also for single variable <p>The previous examples validated complete variable files (with potentially multiple variables), the following example shows how to validate a single variable (the same as above) with a provided JSON schema file. It will also make use of the <code>ansible.utils.validate</code> plugin with additional tasks for a more dense output.</p> TasksJSON Schema <pre><code>- name: Run variable validation\n  ansible.builtin.set_fact:\n    server_list_variable_validation: \"{{ server_list | ansible.utils.validate(validation_criteria, engine='ansible.utils.jsonschema') }}\"\n  vars:\n    validation_criteria: \"{{ lookup('ansible.builtin.file', 'json_schemas/server_list.json') }}\"\n\n- name: Output validation errors for server_list variable\n  ansible.builtin.debug:\n    msg: \"Error in {{ item.data_path }}\"\n  loop: \"{{ server_list_variable_validation }}\"\n  loop_control:\n    label: \"{{ item.message }}\"\n  when: server_list_variable_validation | length &gt; 0\n\n- name: Assert variable validation\n  ansible.builtin.assert:\n    that:\n      - server_list_variable_validation | length == 0\n    quiet: true\n    fail_msg: \"Validation failed, fix the errors shown above!\"\n</code></pre> <p>The validation plugin produces a list of validations. The second task is shown if the list contains entries, the last task fails the playbook by using the assert module if the validation list is not empty.</p> <p>Pretty much the same validation as before, but this time the uppermost type (line) is array (a list).</p> <pre><code>{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"title\": \"Server List variable validation\",\n    \"description\": \"A schema to validate the variable server_list\",\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"additionalProperties\": false,\n        \"properties\": {\n            \"fqdn\": {\n                \"type\": \"string\",\n                \"description\": \"Server name with example.com domain\",\n                \"pattern\": \"^([a-z0-9]+)(\\\\.example\\\\.com)$\"\n            },\n            \"ipv4_address_list\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"string\",\n                    \"format\": \"ipv4\"\n                }\n            },\n            \"cores\": {\n                \"type\": \"number\",\n                \"minimum\": 1,\n                \"maximum\": 64\n            },\n            \"memory\": {\n                \"type\": \"string\",\n                \"description\": \"Memory size in GB, expects a number followed by GB\",\n                \"pattern\": \"^\\\\d+GB$\"\n            },\n            \"disk_space\": {\n                \"type\": \"string\",\n                \"description\": \"Disk size in GB, expects a number followed by GB\",\n                \"pattern\": \"^\\\\d+GB$\"\n            },\n            \"business_owner\": {\n                \"type\": \"string\",\n                \"description\": \"Email address of the responsible person for this server\",\n                \"format\": \"email\"\n            }\n        },\n        \"required\": [\n            \"fqdn\",\n            \"ipv4_address_list\",\n            \"cores\",\n            \"memory\",\n            \"disk_space\"\n        ]\n    }\n}\n</code></pre> <p>The tasks produce the following output, only showing the data path and the violation message as the list item label.</p> <pre><code>TASK [Run variable validation] **************************************************************************************\nok: [localhost]\n\nTASK [Output validation errors for server_list variable] ************************************************************\nok: [localhost] =&gt; (item='server2' does not match '^([a-z0-9]+)(\\\\.example\\\\.com)$') =&gt;\n    msg: Error in 1.fqdn\nok: [localhost] =&gt; (item=8 is not of type 'string') =&gt;\n    msg: Error in 1.memory\n\nTASK [Assert variable validation] ***********************************************************************************\nfatal: [localhost]: FAILED! =&gt;\n    assertion: server_list_variable_validation | length == 0\n    changed: false\n    evaluated_to: false\n    msg: Validation failed, fix the errors shown above!\n\nPLAY RECAP **********************************************************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0\n</code></pre>"},{"location":"ansible/variables/#disable-variable-templating","title":"Disable variable templating","text":"<p>Sometimes, it is necessary to provide special characters like curly braces. The most common use cases include passwords that allow special characters like <code>{</code> or <code>%</code>, and JSON arguments that look like templates but should not be templated.  </p> <pre><code>---\nexamplepassword: !unsafe 234%234{435lkj{{lkjsdf\n</code></pre> <p>Abstract</p> <p>When handling values returned by lookup plugins, Ansible uses a data type called <code>unsafe</code> to block templating. Marking data as unsafe prevents malicious users from abusing Jinja2 templates to execute arbitrary code on target machines. The Ansible implementation <code>!unsafe</code> ensures that these values are never templated. You can use the same unsafe data type in variables you define, to prevent templating errors and information disclosure.</p> <p>For complex variables such as hashes or arrays, use <code>!unsafe</code> on the individual elements, take a look at this example for AWX/AAP automation.</p> <p>For Jinja2 templates this behavior can be achieved with the <code>{% raw %}</code> and <code>{% endraw %}</code> tags. Consider the following template where name_of_receiver_group should be replaced with a variable you set elsewhere, but details contains stuff which should stay as it is:</p> <pre><code>receivers:\n- name: \"{{ name_of_receiver_group }}\"\n  opsgenie_configs:\n  - api_key: 123-123-123-123-123\n    send_resolved: false\n    {% raw %}\n    # protecting the go templates inside the raw section.\n    details: { details: \"{{ .CommonAnnotations.SortedPairs.Values | join \\\" \\\" }}\" }\n    {% endraw %}\n</code></pre>"},{"location":"automation-platform/","title":"Ansible Automation Platform","text":"<p>This topic is split into multiple sections, each section covers a different aspect of using the Ansible Automation Platform.</p> <ul> <li> <p> \u00a0 Credentials</p> <p>Secret handling in AAP</p> </li> <li> <p> \u00a0 Workflows</p> <p>Everything regarding Workflow Job templates</p> </li> </ul>"},{"location":"automation-platform/credentials/","title":"Credentials","text":"<p>Credentials are utilized for authentication when launching Jobs against machines, synchronizing with inventory sources, and importing project content from a version control system.</p> <p>You can grant users and teams the ability to use these credentials, without actually exposing the credential to the user.</p>"},{"location":"automation-platform/credentials/#custom-credentials","title":"Custom Credentials","text":"<p>Although a growing number of credential types are already available, it is possible to define additional custom credential types that works in ways similar to existing ones. For example, you could create a custom credential type that injects an API token for a third-party web service into an environment variable, which your playbook or custom inventory script could consume.</p> <p>For example, to provide login credentials for plugins and modules of the Dell EMC OpenManage Enterprise Collection you need to create a custom credential, as no existing credentials type is available. You can set the environment variables <code>OME_USERNAME</code> and <code>OME_PASSWORD</code> by creating a new AAP credentials type.</p> <p>In the left navigation bar, choose Credential Types and click Add, besides the name you need to fill two fields:</p> Configuration Description Input Which input fields you will make available when creating a credential of this type. Injector What your credential type will provide to the playbook Input Configuration<pre><code>fields:\n  - type: string\n    id: username\n    label: Username\n  - type: string\n    id: password\n    label: Password\n    secret: true\nrequired:\n  - username\n  - password\n</code></pre> Injector Configuration<pre><code>env:\n  OME_USERNAME: \"{{ username }}\"\n  OME_PASSWORD: \"{{ password }}\"\n</code></pre> <p>Warning</p> <p>You are responsible for avoiding collisions in the <code>extra_vars</code>, <code>env</code>, and file namespaces. Also, avoid environment variable or extra variable names that start with <code>ANSIBLE_</code> because they are reserved.</p> <p>Save your credential type, create a new credential of this type and attach it to the Job template with the playbook targeting the OpenManage Enterprise API.</p> <p>An example task may look like this:</p> <pre><code>- name: Retrieve basic inventory of all devices\n  dellemc.openmanage.ome_device_info:\n    hostname: \"{{ ansible_host }}\"\n    username: \"{{ lookup('env', 'OME_USERNAME') }}\"\n    password: \"{{ lookup('env', 'OME_PASSWORD') }}\"\n</code></pre> <p>Tip</p> <p>Depending on the module used, you may leave out the <code>username</code> and <code>password</code> key, environment variables are evaluated first.  Take a look at the module documentation if this is possible, otherwise use the lookup plugin as shown above.</p> <p>Additional information can be found in the Ansible documentation.</p>"},{"location":"automation-platform/credentials/#automation-and-templating","title":"Automation and templating","text":"<p>Creating a custom credential with a playbook can be tricky as you need to provide the special, reserved curly braces character as part of the Injector Configuration. During the playbook run, Ansible will try to template the values which will fail as they are undefined (and you want the literal string representation anyway). Therefore, prefix the values with <code>!unsafe</code> to prevent templating the values.</p> <pre><code>- name: Create custom Credential type for DELL OME\n  awx.awx.credential_type:\n    name: Dell EMC OpenManage Enterprise\n    description: Sets environment variables for logging in to OpenManage Enterprise\n    inputs:\n      fields:\n        - id: username\n          type: string\n          label: Username\n        - id: password\n          type: string\n          label: Password\n          secret: true\n      required:\n        - username\n        - password\n    injectors:\n      env:\n        OME_PASSWORD: !unsafe \"{{ password }}\"\n        OME_USERNAME: !unsafe \"{{ username }}\"\n</code></pre> <p>Take a look at Disable variable templating for additional information.</p>"},{"location":"automation-platform/workflows/","title":"Workflows","text":"<p>Workflows allow you to configure a sequence of disparate job templates (or workflow templates) that may or may not share inventory, playbooks, or permissions.</p>"},{"location":"automation-platform/workflows/#variables-across-workflow-steps","title":"Variables across workflow steps","text":"<p>Transferring information across workflow steps can't be done by the <code>set_fact</code> module, these facts are only available during a normal playbook run. Workflow job template run separate Jobs targeting separate playbooks.</p> <p>Possible Use-case</p> <p>Think of a first workflow step searching for an available IP address in an IPAM tool. The second workflow step can't know this IP before the workflow itself starts, therefore this information needs to be transferred from the first workflow step to the second one.</p> <p>In addition to the workflow <code>extra_vars</code>, jobs ran as part of a workflow can inherit variables in the artifacts dictionary of a parent job in the workflow. These artifacts can be defined by the <code>set_stats</code> module.</p> <p>Info</p> <p>The point of set_stats in workflows is to have a vehicle to pass data via <code>--extra-vars</code> to the next job template.</p>"},{"location":"automation-platform/workflows/#setting-stats","title":"Setting stats","text":"<p>The first playbook (Job Template) in the workflow run defines a variable in the <code>data</code> dictionary.</p> Task of playbook in Workflow node 1<pre><code>- name: Setting stat of free IP address for subsequent workflow step\n  ansible.builtin.set_stats:\n    data:\n      available_ip: \"{{ ipam_returned_ip }}\"\n</code></pre> <p>Bug</p> <p>Do not use the <code>per_host</code> parameter, it breaks the artifacts gathering! You can't provide distinct stats per host (without workarounds).</p>"},{"location":"automation-platform/workflows/#retrieving-stats","title":"Retrieving stats","text":"<p>The second playbook (Job Template) in the workflow run references the variable of the <code>data</code> dictionary.</p> Task of playbook in Workflow node 2<pre><code>- name: Output available IP address from previous workflow step\n  ansible.builtin.debug:\n    msg: \"{{ available_ip }}\"\n</code></pre>"},{"location":"automation-platform/workflows/#display-custom-stats","title":"Display custom stats","text":"<p>Custom stats can be displayed at the playbook recap, you must set <code>show_custom_stats</code> in the <code>[defaults]</code> section of your Ansible configuration file:</p> ansible.cfg<pre><code>[defaults]\nshow_custom_stats = true\n</code></pre> <p>Defining the environment variable <code>ANSIBLE_SHOW_CUSTOM_STATS</code> and setting to <code>true</code> achieves the same behavior.</p> Play recap with custom stats <pre><code>PLAY RECAP *********************************************************************\nlocalhost                  : ok=13    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\nCUSTOM STATS: ******************************************************************\n    RUN: { \"available_ip\": 10.28.13.5\"}\n</code></pre>"},{"location":"development/","title":"Development","text":"<p>This topic is split into four main sections, each section covers a different additional tool to consider when developing your Ansible content.</p> <ul> <li> <p> \u00a0 Version Control</p> <p>Small guide for version controlling playbooks.</p> </li> <li> <p> \u00a0 Linting</p> <p>Installation and usage of the community backed Ansible Best Practice checker.</p> </li> <li> <p> \u00a0 Testing</p> <p>How to test your Ansible content during development.</p> </li> <li> <p> \u00a0 Extending</p> <p>How to create your own custom modules and plugins.</p> </li> <li> <p> \u00a0 Monitoring &amp; Troubleshooting</p> <p>How to monitor your playbook for resource consumption or time taken.</p> </li> </ul>"},{"location":"development/#tools","title":"Tools","text":"<p>Each section above make use of an additional tool to support you during your Ansible content development. In most cases the standalone installation, as well as a custom container-based installation and usage method is described.  </p> <p>The Ansible community provides a Container image bundling all the tools described in the sections above.</p> <pre><code>docker pull quay.io/ansible/creator-ee\n</code></pre> <p>For example you could output the version of the installed tools like this:</p> <pre><code>docker run --rm quay.io/ansible/creator-ee ansible-lint --version\n</code></pre> <pre><code>docker run --rm quay.io/ansible/creator-ee molecule --version\n</code></pre> <p>Take a look into the respective sections for more information and additional usage instructions.</p>"},{"location":"development/extending/","title":"Extending Ansible","text":"<p>Ansible is easily customizable, you can extend Ansible by adding custom modules or plugins. You might wonder whether you need a module or a plugin. Ansible modules are units of code that can control system resources or execute system commands. Ansible provides a module library that you can execute directly on remote hosts or through playbooks. Similar to modules are plugins, which are pieces of code that extend core Ansible functionality. Ansible uses a plugin architecture to enable a rich, flexible, and expandable feature set. It ships with several plugins and lets you easily use your own plugins.</p>"},{"location":"development/extending/#store-custom-content","title":"Store custom content","text":"<p>Custom modules can be stored in the <code>library</code> folder in your project root directory, plugins need to be stored in folders called <code>&lt;plugin type&gt;_plugins</code>, e.g. <code>filter_plugins</code>. These locations are still valid, but it is recommended to store custom content in a collection, this way you have all your custom content in a single location (folder).</p> <p>You can store custom collections with your Ansible project, create it with the ansible-galaxy utility and provide the <code>--init-path</code> parameter. The folder <code>collections/ansible_collections</code> will automatically be picked up by Ansible (although your custom collection is not shown by the <code>ansible-galaxy collection list</code> command, adjust the <code>ansible.cfg</code> for that, take a look into the next subsection).</p> <pre><code>ansible-galaxy collection init computacenter.utils --init-path collections/ansible_collections\n</code></pre> <p>This creates the following structure:</p> <pre><code>collections/\n\u2514\u2500\u2500 ansible_collections\n    \u2514\u2500\u2500 computacenter\n        \u2514\u2500\u2500 utils\n            \u251c\u2500\u2500 README.md\n            \u251c\u2500\u2500 docs\n            \u251c\u2500\u2500 galaxy.yml\n            \u251c\u2500\u2500 plugins\n            \u2502   \u2514\u2500\u2500 README.md\n            \u2514\u2500\u2500 roles\n</code></pre> <p>Create subfolder beneath the <code>plugins</code> folder, <code>modules</code> for modules and e.g. <code>filter</code> for filter plugins. Take a look into the included <code>README.md</code> in the plugins folder. Store your custom content in python files in the respective folders.</p> <p>Tip</p> <p>Only underscores (<code>_</code>) are allowed for filenames inside collections! Naming a file <code>cc-filter-plugins.py</code> will result in an error!</p>"},{"location":"development/extending/#listing-custom-collections","title":"Listing (custom) collections","text":"<p>When storing custom collections alongside your project and you want to list all collections, you need to adjust your Ansible configuration. You will be able to use your custom collection nevertheless, this is more a quality of life change.</p> <p>Adjust the <code>collections_paths</code> parameter in the <code>defaults</code> section of your <code>ansible.cfg</code>:</p> <pre><code>[defaults]\ncollections_paths = ~/.ansible/collections:/usr/share/ansible/collections:./collections\n</code></pre> <p>The first two paths are the default locations for collections, paths are separated with colons.</p> Listing collections <p>Using a custom collection in the project folder <code>test</code> with adjusted configuration file. <pre><code>$ ansible-galaxy collection list\n\n# /home/tgruetz/.ansible/collections/ansible_collections\nCollection        Version\n----------------- -------\nansible.netcommon 4.1.0  \nansible.posix     1.4.0  \nansible.utils     2.8.0  \ncisco.aci         2.3.0  \ncisco.ios         4.2.0  \ncommunity.docker  3.3.2  \ncommunity.general 6.1.0  \n\n# /home/tgruetz/test/collections/ansible_collections\nCollection          Version\n------------------- -------\ncomputacenter.utils 1.0.0\n</code></pre></p>"},{"location":"development/extending/#custom-facts","title":"Custom facts","text":"<p>The <code>setup</code> module in Ansible automatically discovers a standard set of facts about each host. If you want to add custom values to your facts, you can provide permanent custom facts using the <code>facts.d</code> directory or even write a custom facts module.</p>"},{"location":"development/extending/#static-facts","title":"Static facts","text":"<p>The easiest method is to add an <code>.ini</code> file to <code>/etc/ansible/facts.d</code> on the remote host, e.g.</p> /etc/ansible/facts.d/general.fact<pre><code>[owner]\nname=Computacenter AG\ncommunity=Ansible Community\n\n[environment]\nstage=production\n</code></pre> <p>Warning</p> <p>Ensure the file has the <code>.fact</code> extension and is not executable, this will break the <code>ansible.builtin.setup</code> module!</p> <p>For example, running an ad-hoc command against an example host with the custom fact:</p> <pre><code>$ ansible -i inventory test -m ansible.builtin.setup -a filter=ansible_local\nubuntu | SUCCESS =&gt; {\n     \"ansible_facts\": {\n        \"ansible_local\": {\n            \"general\": {\n                \"environment\": {\n                    \"stage\": \"production\"\n                },\n                \"owner\": {\n                    \"community\": \"Ansible Community\",\n                    \"name\": \"Computacenter AG\"\n                }\n            }\n        },\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false\n}\n</code></pre> <p>The parent key for the custom fact is the name of the file, the lower keys are the section names of the ini file.</p> <p>Hint</p> <p>The key in <code>ansible_facts</code> for custom content is always <code>ansible_local</code>, this has nothing to do with running locally.</p>"},{"location":"development/extending/#dynamic-facts","title":"Dynamic facts","text":"<p>You can also use <code>facts.d</code> to execute a script on the remote host, generating dynamic custom facts to the ansible_local namespace. Consider the following points when creating dynamic custom facts:</p> <ul> <li>must return JSON data</li> <li>must have the <code>.fact</code> extension (add the correct Shebang!)</li> <li>is executable by the Ansible connection user</li> <li>dependencies must be installed on the remote host</li> </ul> <p>For example, a custom fact returning information about running or exited Docker containers on the remote host can look like this:</p> /etc/ansible/facts.d/docker-containers.fact<pre><code>#!/usr/bin/env python3\n\n# DEPENDENCY: requires Python module 'docker', install e.g. with 'pip3 install docker' or install 'python3-docker' rpm with package manager\n\nimport json\n\ntry:\n    import docker\nexcept ModuleNotFoundError:\n    print(json.dumps({\"error\": \"Python docker module not found! Install requirements!\"}))\n    raise SystemExit()\n\ntry:\n    client = docker.from_env()\nexcept docker.errors.DockerException:\n    print(json.dumps({\"error\": \"Docker Client not instantiated! Is Docker running?\"}))\n    raise SystemExit()\n\ndef exited_containers():\n    exited_containers = []\n\n    for container in client.containers.list(all=True,filters={\"status\": \"exited\"}):\n        exited_containers.append({\"id\": container.short_id, \"name\": container.name, \"image\": container.image.tags[0]})\n\n    return exited_containers\n\ndef running_containers():\n    running_containers = []\n\n    for container in client.containers.list():\n        running_containers.append({\"id\": container.short_id, \"name\": container.name, \"image\": container.image.tags[0]})\n\n    return running_containers\n\n\ndef main():\n\n    container_facts = {\"running\": running_containers(), \"exited\": exited_containers()}\n    print(json.dumps(container_facts))\n\nif __name__ == '__main__':\n   main()\n</code></pre> <p>The custom fact returns a JSON dictionary with two lists, <code>running</code> and <code>exited</code>. Every list item has the Container ID, name and image.</p> Warning <p>Using the fact requires the Python docker module (mind the <code>import docker</code> statement) and the Docker service running on the target node. Otherwise, an error message is returned, e.g.:</p> <p><pre><code>\"ansible_local\": {\n        \"docker-containers\": {\n            \"error\": \"Python docker module not found! Install requirements!\"\n        }\n    }\n</code></pre> <pre><code>\"ansible_local\": {\n        \"docker-containers\": {\n            \"error\": \"Docker Client not instantiated! Is Docker running?\"\n        }\n    }\n</code></pre></p> <p>Executing fact gathering for example returns this:</p> <pre><code>$ ansible -i inventory test -m setup -a filter=ansible_local\nubuntu | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"ansible_local\": {\n            \"docker-containers\": {\n                \"exited\": [\n                    {\n                        \"id\": \"a6bfc512b842\",\n                        \"image\": \"timgrt/rockylinux8-ansible:latest\",\n                        \"name\": \"rocky-linux\"\n                    }\n                ],\n                \"running\": [\n                    {\n                        \"id\": \"f3731d560625\",\n                        \"image\": \"local/timgrt/ansible-best-practices:latest\",\n                        \"name\": \"ansible-best-practices\"\n                    }\n                ]\n            }\n        },\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false\n}\n</code></pre> <p>In the example, we have one running container and one stopped container.</p> Additional info <p>Running <code>docker ps</code> on the target host <pre><code>$ docker ps -a\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED             STATUS                           PORTS                  NAMES\na6bfc512b842   timgrt/rockylinux8-ansible:latest     \"/usr/lib/systemd/sy\u2026\"   About an hour ago   Exited (137) About an hour ago                          rocky-linux\nf3731d560625   local/timgrt/ansible-best-practices   \"/bin/sh -c 'python \u2026\"   4 hours ago         Up 4 hours                       0.0.0.0:8080-&gt;80/tcp   ansible-best-practices\n</code></pre> Executing the script standalone (using a JSON module for better readability): <pre><code>$ /etc/ansible/facts.d/docker-containers.fact | python3 -m json.tool\n{\n    \"running\": [\n        {\n            \"id\": \"f3731d560625\",\n            \"name\": \"ansible-best-practices\",\n            \"image\": \"local/timgrt/ansible-best-practices:latest\"\n        }\n    ],\n    \"exited\": [\n        {\n            \"id\": \"a6bfc512b842\",\n            \"name\": \"rocky-linux\",\n            \"image\": \"timgrt/rockylinux8-ansible:latest\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"development/extending/#developing-modules","title":"Developing modules","text":"<p>Modules are reusable, standalone scripts that can be used by the Ansible API, the ansible command, or the ansible-playbook command.   Modules provide a defined interface. Each module accepts arguments and returns information to Ansible by printing a JSON string to stdout before exiting. Modules execute on the target system (usually that means on a remote system) in separate processes. Modules are technically plugins, but for historical reasons we do not usually talk about \u201cmodule plugins\u201d.</p> <p>Warning</p> <p>Work in Progress - More description necessary.</p>"},{"location":"development/extending/#developing-plugins","title":"Developing plugins","text":"<p>Plugins extend Ansible\u2019s core functionality and execute on the control node within the /usr/bin/ansible process. Plugins offer options and extensions for the core features of Ansible e.g. transforming data, logging output, connecting to inventory, and more. Take a look into the Ansible Developer Documentation for an overview of the different plugin types.</p> <p>All plugins must</p> <ul> <li>be written in Python (in a compatible version of Python)</li> <li>raise errors (when things go wrong)</li> <li>return strings in unicode (to run through Jinja2)</li> <li>conform to Ansible\u2019s configuration and documentation standards (how to use your plugin)</li> </ul> <p>Depending on the type of plugin you want to create, different considerations need to be taken, the next subsections give a brief overview with a small example. Always use the latest Ansible documentation for additional information.</p> <p>Tip</p> <p>The usage of the FQCN for your Plugin is mandatory!</p>"},{"location":"development/extending/#filter-plugins","title":"Filter plugins","text":"<p>Filter plugins manipulate data. They are a feature of Jinja2 and are also available in Jinja2 templates used by the template module. As with all plugins, they can be easily extended, but instead of having a file for each one you can have several per file.</p> <p>This file may be used as a minimal starting point, it includes a small example:</p> <p>cc_filter_plugins.py</p> <pre><code>from __future__ import absolute_import, division, print_function\n__metaclass__ = type\n\nfrom ansible.errors import AnsibleError# (1)!\nfrom ansible.module_utils.common.text.converters import to_native# (2)!\n\ntry:\n    import netaddr# (3)!\nexcept ImportError as imp_exc:\n    NETADDR_IMPORT_ERROR = imp_exc\nelse:\n    NETADDR_IMPORT_ERROR = None\n\n\ndef sort_ip(unsorted_ip_list):# (4)!\n    # Function sorts a given list of IP addresses\n\n    if NETADDR_IMPORT_ERROR:\n        raise AnsibleError('netaddr library must be installed to use this plugin') from NETADDR_IMPORT_ERROR\n\n    if not isinstance(unsorted_ip_list, list):# (5)!\n        raise AnsibleError(\"Filter needs list input, got '%s'\" % type(unsorted_ip_list))\n    else:\n        try:\n            sorted_ip_list = sorted(unsorted_ip_list, key=netaddr.IPAddress)# (6)!\n        except netaddr.core.AddrFormatError as e:\n            raise AnsibleError('Error from netaddr library, %s' % to_native(e))\n\n    return sorted_ip_list# (7)!\n\n\nclass FilterModule(object): # (8)!\n\n    def filters(self):\n        return {\n            # Sorting list of IP Addresses\n            'sort_ip': sort_ip # (9)!\n        }\n</code></pre> <ol> <li>This is the most generic AnsibleError object, depending on the specific plugin type you\u2019re developing you may want to use different ones.</li> <li>Use this to convert plugin output to convert output into Python\u2019s unicode type (to_text) or for wrapping other exceptions into error messages (to_native).</li> <li>This is a non-standard dependency, the user needs to install this beforehand (e.g. <code>pip3 install netaddr --user</code>), therefore surrounding it with try-except. Document necessary requirements!</li> <li>Example plugin definition, this sorts a given list of IP addresses ( Jinja2 sort filter does not work correctly with IPs), it expects a list.</li> <li>Testing if input is a list, otherwise return an error message. Maybe another error type (e.g. AnsibleFilterTypeError) is more appropriate? What other exceptions need to be caught?</li> <li>This line sorts the list with the built-in Python sorted() library, the key specifies the comparison key for each list element, it uses the netaddr library.</li> <li>The function returns a sorted list of IPs.</li> <li>Main class, this is called by Ansible's PluginLoader.</li> <li>Mapping of filter name and definition, you may call your filter like this: <code>\"{{ ip_list | sort_ip }}\"</code> (this only works when stored in the project root in the folder <code>filter_plugins</code>, otherwise you need to use the FQCN!). Filter name and definition do not need to have the same name.  Add more filter definitions by comma-separation.</li> </ol> <p>The Python file needs to be stored in a collection, e.g.:</p> <pre><code>collections/\n\u2514\u2500\u2500 ansible_collections\n    \u2514\u2500\u2500 computacenter\n        \u2514\u2500\u2500 utils\n            \u251c\u2500\u2500 README.md\n            \u251c\u2500\u2500 docs\n            \u251c\u2500\u2500 galaxy.yml\n            \u251c\u2500\u2500 plugins\n            \u2502   \u251c\u2500\u2500 README.md\n            \u2502   \u2514\u2500\u2500 filter\n            \u2502       \u2514\u2500\u2500 cc_filter_plugins.py\n            \u2514\u2500\u2500 roles\n</code></pre> <p>Now, the filter can be used:</p> <pre><code>sorted_ip_list: \"{{ ip_list | computacenter.utils.sort_ip }}\"\n</code></pre>"},{"location":"development/extending/#inventory-plugins","title":"Inventory plugins","text":"<p>Ansible can pull information from different sources, like ServiceNow, Cisco etc. If your source is not covered with the integrated inventory plugins, you can create your own.</p> <p>For more information take a look at Ansible docs - Developing inventory plugin.</p> <p>Key things to note</p> <ul> <li>The DOCUMENTATION section is required and used by the plugin. Note how the options here reflect exactly the options we specified in the csv_inventory.yaml file in the previous step.</li> <li>The NAME should exactly match the name of the plugin everywhere else.</li> <li>For details on the imports and base classes/helpers take a look at the python code in Github</li> </ul> <p>This file may be used as a minimal starting point, it includes a small example:</p> <p>cc_cisco_prime.py</p> <pre><code>from __future__ import absolute_import, division, print_function\n\n__metaclass__ = type\n\n# (1)!\nDOCUMENTATION = r'''\n    name: cc_cisco_prime\n    author:\n    - Kevin Blase (@FlachDerPlatte)\n    - Jonathan Schmidt (@SchmidtJonathan1)\n    short_description: Inventory source for Cisco Prime API.\n    description:\n    - Builds inventory from Cisco Prime API.\n    - Requires a configuration file ending in C(prime.yml) or C(prime.yaml).\n        See the example section for more details.\n    version_added: 1.0.0\n    extends_documentation_fragment:\n    - ansible.builtin.constructed\n    notes:\n    - Nothing\n    options:\n    plugin:\n        description:\n        - The name of the Cisco Prime API Inventory Plugin.\n        - This should always be C(computacenter.utils.cc_cisco_prime).\n        required: true\n        type: str\n        choices: [ computacenter.utils.cc_cisco_prime ]\n'''\n\n# (2)!\nEXAMPLES = r'''\n    # Inventory File in YAML format\n    plugin: computacenter.utils.cc_cisco_prime\n    api_user: user123\n    api_pass: password123\n    api_host_url: host.domain.tld\n'''\n\nimport requests\nfrom ansible.errors import AnsibleParserError\nfrom ansible.inventory.group import to_safe_group_name\nfrom ansible.plugins.inventory import (\n    BaseInventoryPlugin,\n    Constructable,\n    to_safe_group_name,\n)\n\nclass InventoryModule(BaseInventoryPlugin, Constructable):\n\n    NAME = 'computacenter.utils.cc_cisco_prime'  # used internally by Ansible, it should match the file name but not required\n\n    def verify_file(self, path): # (3)!\n        valid = False\n        if super(InventoryModule, self).verify_file(path):\n            if path.endswith(('prime.yaml', 'prime.yml')):\n                valid = True\n            else:\n                self.display.vvv(\n                    'Skipping due to inventory source not ending in \"prime.yaml\" nor \"prime.yml\"')\n        return valid\n\n    def add_host(self, hostname, host_vars):\n        self.inventory.add_host(hostname, group='all')\n\n        for var_name, var_value in host_vars.items():\n            self.inventory.set_variable(hostname, var_name, var_value)\n\n        strict = self.get_option('strict')\n\n        # Add variables created by the user's Jinja2 expressions to the host\n        self._set_composite_vars(self.get_option('compose'), host_vars, hostname, strict=True)\n\n        # Create user-defined groups using variables and Jinja2 conditionals\n        self._add_host_to_composed_groups(self.get_option('groups'), host_vars, hostname, strict=strict)\n        self._add_host_to_keyed_groups(self.get_option('keyed_groups'), host_vars, hostname, strict=strict)\n...\n</code></pre> <ol> <li>Declare option that are needed in the plugin. More about documentation</li> <li>Example with parameter for a inventory file to run the script.</li> <li>Different methods like verify_file, parse and more. Additional information about class and function here</li> </ol> <p>The Python file needs to be stored in a collection, e.g.:</p> <pre><code>collections/\n\u2514\u2500\u2500 ansible_collections\n    \u2514\u2500\u2500 computacenter\n        \u2514\u2500\u2500 utils\n            \u251c\u2500\u2500 README.md\n            \u251c\u2500\u2500 plugins\n            \u2502   \u251c\u2500\u2500 README.md\n            \u2502   \u2514\u2500\u2500 inventory\n            \u2502       \u2514\u2500\u2500 cc_cisco_prime.py\n            \u2514\u2500\u2500 roles\n</code></pre> <p>To run this script, create a inventory file with the correct entries, as in the examples section of the inventory script.</p> <pre><code># inventory.yml\nplugin: computacenter.utils.cc_cisco_prime\napi_user: \"user123\"\napi_pass: \"password123\"\napi_host_url: \"host.domain.tld\"\n</code></pre> <p>Run your playbook, referencing the custom inventory plugin file:</p> <pre><code>ansible-playbook -i inventory.yml main.yml\n</code></pre>"},{"location":"development/git/","title":"Version Control","text":"<p>Ansible content should be treated as any project containing source code, therefore using version control is always recommended. This guide focuses on Git as it is the most widespread tool.</p>"},{"location":"development/git/#installation","title":"Installation","text":"<p>Most Linux distributions already have Git installed, otherwise install the package with the package manager of the system, for example:</p> <pre><code>sudo yum install git\n</code></pre>"},{"location":"development/git/#configuration","title":"Configuration","text":"<p>Git needs some minimal configuration, most important you need to tell Git who you are.</p> <pre><code>git config --global user.name \"Your Name\"\n</code></pre> <pre><code>git config --global user.email \"your.mail@computacenter.com\"\n</code></pre> <p>Every commit you make can now be traced back to you, this enables collaborating work on Ansible projects.</p>"},{"location":"development/git/#workflow","title":"Workflow","text":"<p>Git has multiple states that your files can reside in:</p> <ul> <li>untracked</li> <li>modified</li> <li>staged</li> <li>committed</li> </ul> <p>The files flow through different sections of your Git project:</p> <ul> <li>Working Directory - also called Working tree, this is basically your filesystem where you are developing</li> <li>Staging Area - also called Index, the files that will go into your next commit</li> <li>Local Repository - the <code>.git</code> folder where metadata and objects are stored for your project.</li> <li>Remote Repository - the (optional, but recommended) upstream repository</li> </ul> <p>Success</p> <p>Although this seems complicated, don't worry, in most cases Git is fairly easy.</p> <p>The basic Git workflow goes something like this:</p> <ol> <li>You modify files in your working tree.</li> <li>You selectively stage just those changes you want to be part of your next commit, which adds only those changes to the staging area.</li> <li>You do a commit, which takes the files as they are in the staging area and stores that snapshot permanently to your Git directory.</li> </ol> <p>The commands you will be using the most and how the files in different states flow through the stages is shown below:</p> <pre><code>sequenceDiagram\n  box Remote\n  participant UR as Upstream Repository\n  end\n  box Local\n  participant LR as Local Repository\n  participant SG as Staging Area\n  participant WS as Working Directory\n  participant SH as Stash\n  end\n  UR-&gt;&gt;WS: git clone\n  UR-&gt;&gt;WS: git pull\n  UR-&gt;&gt;LR: git fetch\n  LR-&gt;&gt;WS: git checkout -b &lt;branch-name&gt;\n  WS-&gt;&gt;SG: git add &lt;file&gt;\n  WS-&gt;&gt;SG: git add -A\n  SG-&gt;&gt;LR: git commit -m \"Commit message\"\n  LR-&gt;&gt;UR: git push\n  WS-&gt;&gt;SH: git stash\n  SH-&gt;&gt;WS: git stash pop</code></pre>"},{"location":"development/git/#branching-concept","title":"Branching concept","text":"<p>Branches are a part of your everyday development process, they are effectively a pointer to a snapshot of your changes. When you want to add a new feature or fix a bug, you spawn a new branch to encapsulate your changes. This makes it harder for unstable code to get merged into the main code base, and it gives you the chance to clean up your future's history before merging it into the main branch. We are using the following branches:</p> <ul> <li>main (protected, only merge commits are allowed)</li> <li>dev (protected, force-pushes are allowed)</li> <li>feature/branch-name</li> <li>bugfix/branch-name</li> <li>hotfix/branch-name</li> </ul> <p>The main branch is the production-code, forking (a feature or bugfix branch) is always done from the dev branch. Forking a hotfix branch is done from the main branch, as it should fix something not working with the production code.</p>"},{"location":"development/git/#feature-request","title":"Feature request","text":"<p>Creating a new feature should be done with a fork of the latest stage of the dev branch, prefix your branch-name with <code>feature/</code> and provide a short, but meaningful description of the new feature.</p> <pre><code>gitGraph\n   commit\n   commit\n   branch dev\n   checkout dev\n   commit\n   branch feature\n   checkout feature\n   commit\n   commit\n   checkout dev\n   commit\n   checkout feature\n   merge dev\n   checkout dev\n   merge feature\n   commit\n   checkout main\n   merge dev\n   checkout dev\n   commit\n   checkout main\n   commit type:HIGHLIGHT</code></pre> <p>The complete workflow with git commands looks something like this:</p> <pre><code>$ git checkout dev\nSwitched to branch 'dev'\nYour branch is behind 'origin/dev' by 3 commits, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\n$ git pull\nUpdating b666be1..e1fc998\nFast-forward\n...\n$ git checkout -b feature/postgres-ha\nSwitched to a new branch 'feature/postgres-ha'\n</code></pre> <p>The single steps in order:</p> <ol> <li><code>git checkout dev</code> - Switching to dev branch.</li> <li><code>git pull</code> - Getting latest changes from upstream dev branch to local dev branch</li> <li><code>git checkout -b feature/postgres-ha</code> - Creating and switching to hotfix branch.</li> </ol> <p>Start developing, save your work in a commit (or multiple commits).</p> <pre><code>$ git status\n...\n$ git add -A\n...\n$ git commit -m \"Added tasks to configure Postgres High-Availability.\"\n</code></pre> <p>As the last step, before pushing your changes to the UR and opening a merge request, ensure that the latest changes from the dev branch (which were made by others during your feature development) are also in your branch and no merge conflicts arise. Do the following steps:</p> <pre><code>$ git checkout dev\nSwitched to branch 'dev'\nYour branch is behind 'origin/dev' by 2 commits, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\n$ git pull\nUpdating e546ag7..klr732i\nFast-forward\n...\n$ git checkout -b feature/postgres-ha\n...\nSwitched to branch 'feature/postgres-ha'\n$ git merge dev\n...\n$ git push -u origin\n</code></pre>"},{"location":"development/git/#bugfix-request","title":"Bugfix request","text":"<p>In case you need to fix a bug in a role or playbook, fork a new branch from dev and prefix your branch-name with <code>bugfix/</code> and provide a short, but meaningful description of the unwanted behavior.  </p> <p>Info</p> <p>The steps are the same as for a feature branch, only the branch-name should indicate that a bug is to be fixed.</p> <pre><code>gitGraph\n   commit\n   commit\n   branch dev\n   checkout dev\n   commit\n   branch bugfix\n   checkout bugfix\n   commit\n   commit\n   checkout dev\n   commit\n   checkout bugfix\n   merge dev\n   checkout dev\n   merge bugfix\n   commit\n   checkout main\n   merge dev\n   checkout dev\n   commit\n   checkout main\n   commit type:HIGHLIGHT</code></pre> <p>Take a look at the section above for an explanation of the single steps.</p>"},{"location":"development/git/#hotfix-request","title":"Hotfix request","text":"<pre><code>gitGraph\n   commit\n   commit\n   branch dev\n   checkout dev\n   commit\n   checkout main\n   commit\n   branch hotfix\n   checkout hotfix\n   commit\n   checkout main\n   checkout hotfix\n   commit\n   checkout main\n   merge hotfix\n   checkout dev\n   merge main\n   commit\n   commit\n   checkout main\n   commit type:HIGHLIGHT</code></pre> <p>The complete workflow with git commands looks something like this:</p> <pre><code>$ git checkout main\nSwitched to branch 'main'\nYour branch is behind 'origin/main' by 11 commits, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\n$ git pull\nUpdating b666be1..e1fc998\nFast-forward\n...\n$ git checkout -b hotfix/mitigate-prod-outage\nSwitched to a new branch 'hotfix/mitigate-prod-outage'\n</code></pre> <p>The single steps in order:</p> <ol> <li><code>git checkout main</code> - Switching to main branch.</li> <li><code>git pull</code> - Getting latest changes from upstream main branch to local main branch</li> <li><code>git checkout -b hotfix/mitigate-prod-outage</code> - Creating and switching to hotfix branch.</li> </ol> <p>After creating (and testing!) the fixes, save your work in a commit (or multiple commits).</p> <pre><code>$ git status\n...\n$ git add -A\n...\n$ git commit -m \"Fixes Issue #31, will restore prod environment.\"\n</code></pre> <p>Now, push your changes to the UR.</p> <pre><code>$ git push -u origin\n...\n</code></pre> <p>In the UR, open a merge request from your hotfix branch to the main branch.</p> <p>Note</p> <p>After rolling out the changes to the production environment and ensuring the hotfix works as expected, open a new merge request against the dev branch to ensure the fixes are also available in the development stage.</p>"},{"location":"development/git/#git-hooks","title":"Git hooks","text":"<p>Git Hooks are scripts that Git can execute automatically when certain events occur, such as before or after a commit, push, or merge. There are several types of Git Hooks, each with a specific purpose.</p>"},{"location":"development/git/#pre-commit","title":"Pre-Commit","text":"<p>Pre-commit hooks can be used to enforce code formatting or run tests before a commit is made.  </p> <p>The most convenient way is the use of the pre-commit framework, install the pre-commit utility:</p> <pre><code>pip3 install pre-commit\n</code></pre> <p>Use the following configuration as a starting point, create the file in your project folder.</p> .pre-commit-config.yaml<pre><code>repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: check-yaml\n      - id: check-merge-conflict\n      - id: trailing-whitespace\n        args: [--markdown-linebreak-ext=md]\n      - id: no-commit-to-branch\n      - id: requirements-txt-fixer\n  - repo: https://github.com/timgrt/pre-commit-hooks\n    rev: v0.2.0\n    hooks:\n      - id: check-file-names\n      - id: check-vault-files\n  - repo: https://github.com/ansible-community/ansible-lint\n    rev: v6.15.0\n    hooks:\n      - id: ansible-lint\n</code></pre> <p>Take a look at https://pre-commit.com/hooks.html for additional hooks for your use-case.  </p> <p>Install all hooks of the <code>.pre-commit-config.yaml</code> file:</p> <pre><code>pre-commit install\n</code></pre> <p>Run the <code>autoupdate</code> command to update all revisions to the latest state:</p> <pre><code>pre-commit autoupdate\n</code></pre> <p>Success</p> <p>pre-commit will now run on every commit.</p> <p>You can run all hooks at any time with the following command, without committing:</p> <pre><code>pre-commit run -a\n</code></pre> Example output <pre><code>$ pre-commit run -a\ncheck yaml...............................................................Passed\ncheck for merge conflicts................................................Passed\ntrim trailing whitespace.................................................Passed\ndon't commit to branch...................................................Passed\nfix requirements.txt.................................(no files to check)Skipped\nmarkdownlint-docker......................................................Passed\nCheck files for non-compliant names......................................Passed\nAnsible-lint.............................................................Failed\n- hook id: ansible-lint\n- exit code: 2\n\n[...output cut for readability...]\n\nRead documentation for instructions on how to ignore specific rule violations.\n\n                      Rule Violation Summary  \ncount tag                           profile rule associated tags  \n    3 role-name                     basic   deprecations, metadata\n    1 name[missing]                 basic   idiom  \n    2 yaml[comments]                basic   formatting, yaml  \n    1 yaml[new-line-at-end-of-file] basic   formatting, yaml  \n\nFailed after min profile: 7 failure(s), 0 warning(s) on 30 files.\n</code></pre> <p>Hint</p> <p>The first time pre-commit runs on a file it will automatically download, install, and run the hook. Note that running a hook for the first time may be slow. but will be faster in subsequent iterations.</p>"},{"location":"development/git/#offline","title":"Offline","text":"<p>The pre-commit framework by default needs internet connection to setup the hooks, in disconnected environments you can build the pre-commit hook yourself.</p> <p>The following script can be used as a starting point, it uses ansible-lint from inside a container (see Lint in Docker Image how to build it) and also checks for unencrypted files in your commit.</p> .git/hooks/pre-commit <pre><code>#!/bin/bash\n#\n# File should be .git/hooks/pre-commit and executable\n#\n\n# Pre-commit hook that runs ansible-lint Container for best practice checking\n# If lint has errors, commit will fail with an error message.\nif [[ ! $(docker inspect ansible-lint) ]] ; then\n  echo \"# DOCKER IMAGE NOT FOUND\"\n  echo \"# Build the Docker image from the Gitlab project 'ansible-lint Docker Image'.\"\n  echo \"# No linting is done!\"\nelse\n  echo \"# Running 'ansible-lint' against commit, this takes some time ...\"\n  # Getting all files currently staged and storing them in variable\n  FILES_TO_LINT=$(git diff --cached --name-only)\n  # Running with shared profile, see https://ansible-lint.readthedocs.io/profiles/\n  if [ -z \"$FILES_TO_LINT\" ] ; then\n    echo \"# No files linting found. Add files to SG area with 'git add &lt;file&gt;'.\"\n  else\n    docker run --rm -v $(pwd):/data ansible-lint $FILES_TO_LINT\n    if [ ! $? = 0 ]; then\n      echo \"# COMMIT REJECTED\"\n      echo \"# Please fix the shown linting errors\"\n      echo \"#   (or force the commit with '--no-verify').\"\n      exit 1;\n    fi\n  fi\nfi\n\n# Pre-commit hook that verifies if all files containing 'vault' in the name\n# are encrypted.\n# If not, commit will fail with an error message.\n# Finds all files in 'inventory' folder or 'files' folder in roles. Files in other\n# locations are not recognized!\nFILES_PATTERN='(inventory.*vault.*)|(files.*vault.*)'\nREQUIRED='ANSIBLE_VAULT'\n\nEXIT_STATUS=0\nwipe=\"\\033[1m\\033[0m\"\nyellow='\\033[1;33m'\n# carriage return hack. Leave it on 2 lines.\ncr='\n'\necho \"# Checking for unencrypted vault files in commit ...\"\nfor f in $(git diff --cached --name-only | grep -E $FILES_PATTERN)\ndo\n  # test for the presence of the required bit.\n  MATCH=`head -n1 $f | grep --no-messages $REQUIRED`\n  if [ ! $MATCH ] ; then\n    # Build the list of unencrypted files if any\n    UNENCRYPTED_FILES=\"$f$cr$UNENCRYPTED_FILES\"\n    EXIT_STATUS=1\n  fi\ndone\nif [ ! $EXIT_STATUS = 0 ] ; then\n  echo '# COMMIT REJECTED'\n  echo '# Looks like unencrypted ansible-vault files are part of the commit:'\n  echo '#'\n  while read -r line; do\n    if [ -n \"$line\" ] ; then\n      echo -e \"#\\t${yellow}unencrypted:   $line${wipe}\"\n    fi\n  done &lt;&lt;&lt; \"$UNENCRYPTED_FILES\"\n  echo '#'\n  echo \"# Please encrypt them with 'ansible-vault encrypt &lt;file&gt;'\"\n  echo \"#   (or force the commit with '--no-verify').\"\n  exit $EXIT_STATUS\nfi\nexit $EXIT_STATUS\n</code></pre>"},{"location":"development/linting/","title":"Linting","text":"<p>Ansible Lint is a best-practice checker for Ansible, maintained by the Ansible community.</p>"},{"location":"development/linting/#installation","title":"Installation","text":"<p>Ansible Lint is installed through the Python packet manager:</p> <p>Note</p> <p>Ansible Lint always needs Ansible itself, ansible-core is enough.</p> <pre><code>pip3 install ansible-lint\n</code></pre>"},{"location":"development/linting/#configuration","title":"Configuration","text":"<p>Minimal configuration is necessary, use the following as a starting point in your project directory:</p> .ansible-lint<pre><code>---\nprofile: shared\n\n# Silence infos, warnings and don't show summary\nquiet: true\n\nskip_list:\n  - role-name\n\n# Enable some useful rules which are opt-in\nenable_list:\n  - args\n  - empty-string-compare\n  - no-log-password\n  - no-same-owner\n</code></pre> <p>Profiles gradually increase the strictness of rules, from lowest to highest, every profile extends to previous:</p> Strictness Profile name Description 1 min ensures that Ansible can load content, rules in this profile are mandatory 2 basic prevents common coding issues and enforces standard styles and formatting 3 moderate ensures that content adheres to best practices for making content easier to read and maintain 4 safety avoids module calls that can have non-determinant outcomes or security concerns 5 shared for packaging and publishing to galaxy.ansible.com, automation-hub, or a private instance 6 production for inclusion in AAP as validated or certified content <p>Take a look at the official documentation for more information.</p>"},{"location":"development/linting/#usage","title":"Usage","text":"<p>The usage is fairly simple, just run <code>ansible-lint &lt;your-playbook&gt;</code>. The tool will check your playbook for best-practices, it traverses your playbook and will lint all included playbooks and roles.</p> <p>Take a look at the ansible-lint documentation for additional information.</p>"},{"location":"development/linting/#lint-in-docker-image","title":"Lint in Docker Image","text":"<p>The following Dockerfile can be used to build a Docker Container image which bundles ansible-lint and its dependencies:</p> Dockerfile <pre><code>FROM python:3.9-slim\n\n# Enable colored output\nENV TERM xterm-256color\n\n# Defining Ansible environment variable to not output deprecation warnings. This is not useful in the linting container.\n# This overwrites the value in the ansible.cfg from volume mount\nENV ANSIBLE_DEPRECATION_WARNINGS=false\n\n# Install requirements.\nRUN apt-get update &amp;&amp; apt-get install -y \\\n  git \\\n  &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Update pip\nRUN python3 -m pip install --no-cache-dir --no-compile --upgrade pip\n\n# Install ansible-lint and dependencies\nRUN pip3 install --no-cache-dir --no-compile ansible-lint ansible yamllint\n\nWORKDIR /data\nENTRYPOINT [\"ansible-lint\"]\nCMD [\"--version\"]\n</code></pre> <p>Build the container image, the command expects that the Dockerfile is present in the current directory:</p> <pre><code>docker build -t ansible-lint .\n</code></pre> <p>After building the image, the image can be used. Inside of the Ansible project directory, run this command (e.g. this lints the <code>site.yml</code> playbook).</p> <pre><code>docker run --rm -v $(pwd):/data ansible-lint site.yml\n</code></pre> <p>The output for example is something like this, ansible-lint reports a warning regarding unnecessary white-spaces in a line, as well as an error regarding unset file permissions (fix could be setting <code>mode: 0644</code> in the task):</p> <pre><code>$ docker run --rm -v $(pwd):/data ansible-lint site.yml\nWARNING  Overriding detected file kind 'yaml' with 'playbook' for given positional argument: site.yml\nWARNING  Listing 2 violation(s) that are fatal\nyaml: trailing spaces (trailing-spaces)\nroles/network/tasks/cacheserve-loopback-interface.yml:19\n\nrisky-file-permissions: File permissions unset or incorrect\nroles/network/tasks/cacheserve-loopback-interface.yml:43 Task/Handler: Deploy loopback interface config for Cacheserve\n\nYou can skip specific rules or tags by adding them to your configuration file:\n# .ansible-lint\nwarn_list:  # or 'skip_list' to silence them completely\n  - experimental  # all rules tagged as experimental\n  - yaml  # Violations reported by yamllint\n\nFinished with 1 failure(s), 1 warning(s) on 460 files.\n</code></pre> <p>To simplify the usage, consider adding an alias to your <code>.bashrc</code>, e.g.:</p> <pre><code># .bashrc\n# User specific aliases and functions\nalias lint=\"docker run --rm -v $(pwd):/data ansible-lint\"\n</code></pre> <p>After running <code>source ~/.bashrc</code> you can use the alias:</p> <pre><code>lint site.yml\n</code></pre>"},{"location":"development/linting/#automated-linting","title":"Automated Linting","text":"<p>Lining can and should be done automatically, this way you can't forget to check your playbook for best practices. This can be done on multiple levels, either locally as part of your Git workflow, as well as with a pipeline in your remote repository.</p>"},{"location":"development/linting/#git-pre-commit-hook","title":"Git pre-commit hook","text":"<p>A nice way to check for best practices during your Git workflow is the usage of a pre-commit hook. These hooks can be simple bash script, which are run whenever you are committing changes locally to the staging area or a framework/utility like pre-commit.</p> <p>Take a look at the Version Control section for installing and configuring pre-commit hooks.</p>"},{"location":"development/linting/#ci-pipeline","title":"CI Pipeline","text":"<p>Running ansible-lint through a CI pipeline automatically when merging changes to the Git repository is highly advisable.</p> <p>A possible pipeline in Gitlab may look like this, utilizing the container image above:</p> <pre><code>workflow:\n  rules:\n    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'\n    - if: $CI_PIPELINE_SOURCE == 'web'\n    - if: $CI_PIPELINE_SOURCE == 'schedule'\n\nvariables:\n  GIT_STRATEGY: clone\n\nstages:\n  - prepare\n  - syntax\n  - lint\n\nprepare:\n  stage: prepare\n  script:\n    - 'echo -e \"### Prepare playbook execution. ###\"'\n    - 'cp ansible.cfg.sample-lab ansible.cfg'\n    - 'echo -e \"$VAULT_PASSWORD\" &gt; .vault-password'\n  artifacts:\n    paths:\n      - ansible.cfg\n      - .vault-password\n  cache:\n    paths:\n      - ansible.cfg\n      - .vault-password\n  tags:\n    - ansible-lint\n\nsyntax-check:\n  stage: syntax\n  script:\n    - 'echo -e \"Perform a syntax check on the playbook. ###\"'\n    - 'docker run --rm --entrypoint ansible-playbook -v $(pwd):/data ansible-lint site.yml --syntax-check'\n  cache:\n    paths:\n      - ansible.cfg\n      - .vault-password\n  dependencies:\n    - prepare\n  tags:\n    - ansible-lint\n\nansible-lint:\n  stage: lint\n  script:\n    - 'echo -e \"### Check for best practices with ansible-lint. ###\"'\n    - 'echo -e \"### Using ansible-lint version: ###\"'\n    - 'docker run --rm -v $(pwd):/data ansible-lint'\n    - 'docker run --rm -v $(pwd):/data ansible-lint site.yml'\n  cache:\n    paths:\n      - ansible.cfg\n      - .vault-password\n  dependencies:\n    - prepare\n  tags:\n    - ansible-lint\n</code></pre> <p>If you want to utilize the installed ansible and ansible-lint utilities on the host running the Gitlab Runner change the commands in the syntax stage to <code>ansible-playbook site.yml --syntax-check</code> and in the lint stage to <code>ansible-lint --version</code> and <code>ansible-lint site.yml</code>.</p>"},{"location":"development/monitoring/","title":"Monitoring &amp; Troubleshooting","text":"<p>This section describes different methods to monitor or troubleshoot your Ansible playbook runs. When you need metrics about playbook execution and machine resource consumption, callback plugins can help you drill down into the data and troubleshoot issues.</p>"},{"location":"development/monitoring/#how-long-does-it-take","title":"How long does it take?","text":"<p>To measure the time spent for tasks and the overall playbook run, multiple callback plugins are available. Install the necessary collections which include the desired callback plugins:</p> <pre><code>ansible-galaxy collection install ansible.posix\n</code></pre> <p>The following plugins are available and useful for different purposes.</p> <ul> <li>ansible.posix.timer - Adds total play duration to the play stats.</li> <li>ansible.posix.profile_tasks - For timing individual tasks and overall execution time.</li> <li>ansible.posix.profile_roles - Adds timing information to roles.</li> </ul> <p>Tip</p> <p>To use the callback plugins, they need to be enabled.</p> <p>For example, to show the start-time and duration for every task, you can use the <code>timer</code> and <code>profile_tasks</code> callback plugin. Add the following block to your <code>ansible.cfg</code>:</p> <pre><code>[defaults]\ncallbacks_enabled = ansible.posix.timer, ansible.posix.profile_tasks\n</code></pre> Example output <pre><code>$ ansible-playbook -i inventory.ini create_workshop_environment.yml\n\nPLAY [Create Workshop environment] ****************************************************************************************************\n\nTASK [Gathering Facts] ****************************************************************************************************************\nSaturday 07 September 2024  16:05:19 +0200 (0:00:00.004)       0:00:00.004 ****\nok: [localhost]\n\nTASK [Get package facts] **************************************************************************************************************\nSaturday 07 September 2024  16:05:20 +0200 (0:00:00.836)       0:00:00.840 ****\nok: [localhost]\n\n[...cut for readability...]\n\nPLAY RECAP ****************************************************************************************************************************\nlocalhost                  : ok=10   changed=6    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0  \nnode1                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \nnode2                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \nnode3                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n\nPlaybook run took 0 days, 0 hours, 0 minutes, 43 seconds\nSaturday 07 September 2024  16:06:03 +0200 (0:00:02.318)       0:00:43.633 ****\n===============================================================================\nInstall SSH daemon ------------------------------------------------------------------------------------------------------------ 25.25s\nStart managed node containers, publish 3 ports for each container -------------------------------------------------------------- 3.67s\nGathering Facts ---------------------------------------------------------------------------------------------------------------- 2.92s\nStart SSH daemon --------------------------------------------------------------------------------------------------------------- 2.64s\nAdd public key of workshop SSH keypair to authorized_keys of ansible user ------------------------------------------------------ 2.32s\nRemove /run/nologin to be able to login as unprivileged user ------------------------------------------------------------------- 2.20s\nCreate OpenSSH keypair for accessing managed nodes ----------------------------------------------------------------------------- 1.38s\nGet package facts -------------------------------------------------------------------------------------------------------------- 0.84s\nGathering Facts ---------------------------------------------------------------------------------------------------------------- 0.84s\nPull image for managed node containers ----------------------------------------------------------------------------------------- 0.52s\nCreate workshop inventory file ------------------------------------------------------------------------------------------------- 0.28s\nDeploy ansible.cfg to home directory ------------------------------------------------------------------------------------------- 0.19s\nCreate folder for workshop inventory ------------------------------------------------------------------------------------------- 0.18s\nAdd block to ssh_config for easy SSH access to managed nodes ------------------------------------------------------------------- 0.17s\nCheck for existing SSH keypair ------------------------------------------------------------------------------------------------- 0.14s\nInstall Podman ----------------------------------------------------------------------------------------------------------------- 0.03s\nBackup file of .ansible.cfg created -------------------------------------------------------------------------------------------- 0.02s\nCheck if OpenSSH keypair does not match target configuration ------------------------------------------------------------------- 0.02s\nAbort playbook if keypair was found and does not match target configuration ---------------------------------------------------- 0.02s\n</code></pre>"},{"location":"development/monitoring/#how-much-resources-are-consumed","title":"How much resources are consumed?","text":"<p>To measure system resources used by Ansible, you can use the following callback plugins, both are utilizing cgroups.</p> <ul> <li>community.general.cgroup_memory_recap - profiles maximum memory usage of individual tasks and displays a recap at the end</li> <li>ansible.posix.cgroup_perf_recap - profiles system activity of Ansible and individual tasks and displays a recap at the end of the playbook execution.</li> </ul> <p>cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, etc) of a collection of processes. You can use the cgroup-tools (for Fedora-based systems the package is called libcgroup-tools) utilities to create a cgroup profile and interact with cgroups.</p> <p>Warning</p> <p>Installing <code>cgroup-tools</code> and creating the cgroup-profile requires sudo permissions.</p> <p>Install the cgroup-tools which contains command-line programs, services and a daemon for manipulating control groups using the libcgroup library.</p> <pre><code>sudo apt install cgroup-tools\n</code></pre> <p>Create a cgroup which includes the CPU Accounting, the memory (RAM) and the PIDs subsystem:</p> <pre><code>sudo cgcreate -a ${USER}:${USER} -t ${USER}:${USER} -g cpuacct,memory,pids:ansible_profile\n</code></pre> <p>Install the necessary collections which include the desired callback plugins:</p> <pre><code>ansible-galaxy collection install ansible.posix community.general\n</code></pre> <p>Tip</p> <p>To use the callback plugins, they need to be enabled and configured.</p>"},{"location":"development/monitoring/#show-ram-usage","title":"Show RAM usage","text":"<p>To show the memory usage for every task, you can use the <code>cgroup_memory_recap</code> callback plugin. Add the following block to your <code>ansible.cfg</code>:</p> <pre><code>[defaults]\ncallbacks_enabled = community.general.cgroup_memory_recap\n\n[callback_cgroupmemrecap]\ncur_mem_file = /sys/fs/cgroup/memory/ansible_profile/memory.usage_in_bytes\nmax_mem_file = /sys/fs/cgroup/memory/ansible_profile/memory.max_usage_in_bytes\n</code></pre> <p>The cgexec program executes a task command (in our case a playbook run) with arguments in given control groups (in our case the memory group only).</p> <pre><code>cgexec -g memory:ansible_profile ansible-playbook playbook.yml\n</code></pre> Example output <pre><code>$ cgexec -g memory:ansible_profile ansible-playbook -i inventory.ini create_workshop_environment.yml\n\nPLAY [Create Workshop environment] ******************************************************\n\nTASK [Gathering Facts] ******************************************************************\nok: [localhost]\n\nTASK [Get package facts] ****************************************************************\nok: [localhost]\n\n[...cut for readability...]\n\nPLAY RECAP ******************************************************************************\nlocalhost                  : ok=10   changed=6    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0  \nnode1                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \nnode2                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \nnode3                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n\n\nCGROUP MEMORY RECAP *********************************************************************\nExecution Maximum: 281.57MB\n\nGathering Facts (299e2579-3d81-65cc-ccd9-00000000001f): 148.23MB\nGet package facts (299e2579-3d81-65cc-ccd9-000000000006): 220.73MB\nInstall Podman (299e2579-3d81-65cc-ccd9-000000000007): 166.30MB\nPull image for managed node containers (299e2579-3d81-65cc-ccd9-000000000008): 220.42MB\nStart managed node containers, publish 3 ports for each container (299e2579-3d81-65cc-ccd9-000000000009): 227.33MB\nCreate folder for workshop inventory (299e2579-3d81-65cc-ccd9-00000000000a): 190.53MB\nCreate workshop inventory file (299e2579-3d81-65cc-ccd9-00000000000b): 203.59MB\nAdd block to ssh_config for easy SSH access to managed nodes (299e2579-3d81-65cc-ccd9-00000000000c): 192.20MB\nDeploy ansible.cfg to home directory (299e2579-3d81-65cc-ccd9-00000000000d): 185.89MB\nBackup file of .ansible.cfg created (299e2579-3d81-65cc-ccd9-00000000000e): 168.18MB\nCheck for existing SSH keypair (299e2579-3d81-65cc-ccd9-00000000000f): 191.01MB\nCheck if OpenSSH keypair does not match target configuration (299e2579-3d81-65cc-ccd9-000000000011): 168.10MB\nAbort playbook if keypair was found and does not match target configuration (299e2579-3d81-65cc-ccd9-000000000012): 168.20MB\nCreate OpenSSH keypair for accessing managed nodes (299e2579-3d81-65cc-ccd9-000000000014): 210.39MB\nGathering Facts (299e2579-3d81-65cc-ccd9-000000000060): 251.42MB\nInstall SSH daemon (299e2579-3d81-65cc-ccd9-000000000017): 275.68MB\nStart SSH daemon (299e2579-3d81-65cc-ccd9-000000000018): 281.44MB\nRemove /run/nologin to be able to login as unprivileged user (299e2579-3d81-65cc-ccd9-000000000019): 250.57MB\nAdd public key of workshop SSH keypair to authorized_keys of ansible user (299e2579-3d81-65cc-ccd9-00000000001a): 273.89MB\n</code></pre> <p>Tip</p> <p>Create an alias for the cgexec... part:</p> <p>~/.bash_aliases</p> <pre><code>alias ansible-playbook-profile='cgexec -g memory:ansible_profile ansible-playbook'\n</code></pre> <p>First time usage requires <code>source ~/.bash_aliases</code>, now you can run:</p> <pre><code>ansible-playbook-profile -i inventory playbook.yml\n</code></pre>"},{"location":"development/monitoring/#show-ram-cpu-pids-usage","title":"Show RAM, CPU &amp; PIDs usage","text":"<p>To show the memory and CPU usage, as well as forked processes for every task, you can use the <code>cgroup_perf_recap</code> callback plugin. Add the following block to your <code>ansible.cfg</code>:</p> <pre><code>[defaults]\ncallbacks_enabled = ansible.posix.cgroup_perf_recap\n\n[callback_cgroup_perf_recap]\ncontrol_group = ansible_profile\n</code></pre> <p>The cgexec program executes a task command (in our case a playbook run) with arguments in given control groups.</p> <pre><code>cgexec -g cpuacct,memory,pids:ansible_profile ansible-playbook playbook.yml\n</code></pre> Example output <pre><code>$ cgexec -g cpuacct,memory,pids:ansible_profile ansible-playbook -i inventory.ini create_workshop_environment.yml\n\nPLAY [Create Workshop environment] *****************************************************************************\n\nTASK [Gathering Facts] *****************************************************************************************\nok: [localhost]\n\nTASK [Get package facts] ***************************************************************************************\nok: [localhost]\n\n[...cut for readability...]\n\nPLAY RECAP *****************************************************************************************************\nlocalhost                  : ok=10   changed=6    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0  \nnode1                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \nnode2                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \nnode3                      : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n\n\nCGROUP PERF RECAP **********************************************************************************************\nMemory Execution Maximum: 286.29MB\ncpu Execution Maximum: 302.46%\npids Execution Maximum: 43.00\n\nmemory:\nGathering Facts (299e2579-3d81-800b-f0f1-00000000001f): 109.20MB\nGet package facts (299e2579-3d81-800b-f0f1-000000000006): 182.14MB\nInstall Podman (299e2579-3d81-800b-f0f1-000000000007): 120.23MB\nPull image for managed node containers (299e2579-3d81-800b-f0f1-000000000008): 216.32MB\nStart managed node containers, publish 3 ports for each container (299e2579-3d81-800b-f0f1-000000000009): 224.69MB\nCreate folder for workshop inventory (299e2579-3d81-800b-f0f1-00000000000a): 159.62MB\nCreate workshop inventory file (299e2579-3d81-800b-f0f1-00000000000b): 206.01MB\nAdd block to ssh_config for easy SSH access to managed nodes (299e2579-3d81-800b-f0f1-00000000000c): 162.30MB\nDeploy ansible.cfg to home directory (299e2579-3d81-800b-f0f1-00000000000d): 162.27MB\nBackup file of .ansible.cfg created (299e2579-3d81-800b-f0f1-00000000000e): 162.33MB\nCheck for existing SSH keypair (299e2579-3d81-800b-f0f1-00000000000f): 162.94MB\nCheck if OpenSSH keypair does not match target configuration (299e2579-3d81-800b-f0f1-000000000011): 163.47MB\nAbort playbook if keypair was found and does not match target configuration (299e2579-3d81-800b-f0f1-000000000012): 166.45MB\nCreate OpenSSH keypair for accessing managed nodes (299e2579-3d81-800b-f0f1-000000000014): 216.06MB\nGathering Facts (299e2579-3d81-800b-f0f1-000000000060): 250.53MB\nInstall SSH daemon (299e2579-3d81-800b-f0f1-000000000017): 271.96MB\nStart SSH daemon (299e2579-3d81-800b-f0f1-000000000018): 268.99MB\nRemove /run/nologin to be able to login as unprivileged user (299e2579-3d81-800b-f0f1-000000000019): 246.32MB\nAdd public key of workshop SSH keypair to authorized_keys of ansible user (299e2579-3d81-800b-f0f1-00000000001a): 273.55MB\n\ncpu:\nGathering Facts (299e2579-3d81-800b-f0f1-00000000001f): 92.82%\nGet package facts (299e2579-3d81-800b-f0f1-000000000006): 101.37%\nInstall Podman (299e2579-3d81-800b-f0f1-000000000007): 0.00%\nPull image for managed node containers (299e2579-3d81-800b-f0f1-000000000008): 77.08%\nStart managed node containers, publish 3 ports for each container (299e2579-3d81-800b-f0f1-000000000009): 82.08%\nCreate folder for workshop inventory (299e2579-3d81-800b-f0f1-00000000000a): 0.00%\nCreate workshop inventory file (299e2579-3d81-800b-f0f1-00000000000b): 101.61%\nAdd block to ssh_config for easy SSH access to managed nodes (299e2579-3d81-800b-f0f1-00000000000c): 0.00%\nDeploy ansible.cfg to home directory (299e2579-3d81-800b-f0f1-00000000000d): 0.00%\nBackup file of .ansible.cfg created (299e2579-3d81-800b-f0f1-00000000000e): 0.00%\nCheck for existing SSH keypair (299e2579-3d81-800b-f0f1-00000000000f): 0.00%\nCheck if OpenSSH keypair does not match target configuration (299e2579-3d81-800b-f0f1-000000000011): 0.00%\nAbort playbook if keypair was found and does not match target configuration (299e2579-3d81-800b-f0f1-000000000012): 0.00%\nCreate OpenSSH keypair for accessing managed nodes (299e2579-3d81-800b-f0f1-000000000014): 101.40%\nGathering Facts (299e2579-3d81-800b-f0f1-000000000060): 144.79%\nInstall SSH daemon (299e2579-3d81-800b-f0f1-000000000017): 302.46%\nStart SSH daemon (299e2579-3d81-800b-f0f1-000000000018): 245.07%\nRemove /run/nologin to be able to login as unprivileged user (299e2579-3d81-800b-f0f1-000000000019): 151.99%\nAdd public key of workshop SSH keypair to authorized_keys of ansible user (299e2579-3d81-800b-f0f1-00000000001a): 175.70%\n\npids:\nGathering Facts (299e2579-3d81-800b-f0f1-00000000001f): 9.00\nGet package facts (299e2579-3d81-800b-f0f1-000000000006): 9.00\nInstall Podman (299e2579-3d81-800b-f0f1-000000000007): 8.00\nPull image for managed node containers (299e2579-3d81-800b-f0f1-000000000008): 21.00\nStart managed node containers, publish 3 ports for each container (299e2579-3d81-800b-f0f1-000000000009): 22.00\nCreate folder for workshop inventory (299e2579-3d81-800b-f0f1-00000000000a): 9.00\nCreate workshop inventory file (299e2579-3d81-800b-f0f1-00000000000b): 11.00\nAdd block to ssh_config for easy SSH access to managed nodes (299e2579-3d81-800b-f0f1-00000000000c): 8.00\nDeploy ansible.cfg to home directory (299e2579-3d81-800b-f0f1-00000000000d): 12.00\nBackup file of .ansible.cfg created (299e2579-3d81-800b-f0f1-00000000000e): 9.00\nCheck for existing SSH keypair (299e2579-3d81-800b-f0f1-00000000000f): 11.00\nCheck if OpenSSH keypair does not match target configuration (299e2579-3d81-800b-f0f1-000000000011): 11.00\nAbort playbook if keypair was found and does not match target configuration (299e2579-3d81-800b-f0f1-000000000012): 14.00\nCreate OpenSSH keypair for accessing managed nodes (299e2579-3d81-800b-f0f1-000000000014): 17.00\nGathering Facts (299e2579-3d81-800b-f0f1-000000000060): 41.00\nInstall SSH daemon (299e2579-3d81-800b-f0f1-000000000017): 43.00\nStart SSH daemon (299e2579-3d81-800b-f0f1-000000000018): 33.00\nRemove /run/nologin to be able to login as unprivileged user (299e2579-3d81-800b-f0f1-000000000019): 29.00\nAdd public key of workshop SSH keypair to authorized_keys of ansible user (299e2579-3d81-800b-f0f1-00000000001a): 37.00\n</code></pre> <p>Tip</p> <p>Create an alias for the cgexec... part:</p> <p>~/.bash_aliases</p> <pre><code>alias ansible-playbook-profile='cgexec -g cpuacct,memory,pids:ansible_profile ansible-playbook'\n</code></pre> <p>First time usage requires <code>source ~/.bash_aliases</code>, now you can run:</p> <pre><code>ansible-playbook-profile -i inventory playbook.yml\n</code></pre>"},{"location":"development/testing/","title":"Testing","text":""},{"location":"development/testing/#testing","title":"Testing","text":"<p>With many people contributing to the automation, it is crucial to test the automation content in-depth. So when you\u2019re developing new Ansible Content like playbooks, roles and collections, it\u2019s a good idea to test the content in a test environment before using it to automate production infrastructure. Testing ensures the automation works as designed and avoids unpleasant surprises down the road. Testing automation content is often a challenge, since it requires the deployment of specific testing infrastructure as well as setting up the testing conditions to ensure the tests are relevant.</p> <p>Consider the following list for testing your Ansible content, with increasing complexity:</p> <ol> <li>yamllint</li> <li>ansible-playbook --syntax-check</li> <li>ansible-lint</li> <li>molecule test</li> <li>ansible-playbook --check (against production)</li> <li>Parallel infrastructure</li> </ol>"},{"location":"development/testing/#syntax-check","title":"Syntax check","text":"<p>The whole playbook (and all roles and tasks) need to, minimally, pass a basic ansible-playbook syntax check run.</p> <pre><code>ansible-playbook main.yml --syntax-check\n</code></pre> <p>Running this as a step in a CI Pipeline is advisable.</p>"},{"location":"development/testing/#linting","title":"Linting","text":"<p>Take a look at the Linting section for further information.</p>"},{"location":"development/testing/#molecule","title":"Molecule","text":"<p>The Molecule project is designed to aid in the development and testing of Ansible roles, provides support for testing with multiple instances, operating systems and distributions, virtualization providers, test frameworks and testing scenarios. Molecule is mostly used to test roles in isolation (although it is possible to test multiple roles or playbooks at once). To test against a fresh system, molecule uses a container runtime to provision virtualized/containerized test hosts, runs commands on them and asserts the success. By default, Containers don't allow services to be installed, started and stopped as in a virtual machine. We will be using custom systemd-enabled images, which are designed to run an init system as PID 1 for running multi-services inside the container. Also, some additional configuration is needed in the Molecule configuration file as shown below.</p> <p>Take a look at the Molecule documentation for a full overview.</p>"},{"location":"development/testing/#installation","title":"Installation","text":"<p>The described configuration below expects the Podman container runtime on the Ansible Controller (other drivers like Docker are available). You can install Podman with the following command:</p> <pre><code>sudo apt install podman\n</code></pre> <p>The Molecule binary and dependencies are installed through the Python package manager, you'll need a fairly new Python version (Python &gt;= 3.10 with ansible-core &gt;= 2.12). Use a Python Virtual environment (requires the <code>python3-venv</code> package) to encapsulate the installation from the rest of your Controller.</p> <pre><code>python3 -m venv molecule-venv\n</code></pre> <p>Activate the VE:</p> <pre><code>source molecule-venv/bin/activate\n</code></pre> <p>Install dependencies, after upgrading pip:</p> <pre><code>pip3 install --upgrade pip setuptools\n</code></pre> <pre><code>pip3 install ansible-core molecule molecule-plugins[podman]\n</code></pre> <p>Molecule plugins contains the following provider:</p> <ul> <li>azure</li> <li>containers</li> <li>docker</li> <li>ec2</li> <li>gce</li> <li>podman</li> <li>vagrant</li> </ul> <p>Note</p> <p>The Molecule Podman provider requires the modules of the containers.podman collection (as it provisions the containers with Ansible itself). If you only installed <code>ansible-core</code>, you'll need to install the collection separately:</p> <pre><code>ansible-galaxy collection install containers.podman\n</code></pre> <p>If you are done with Molecule testing, use <code>deactivate</code> to leave your VE.</p>"},{"location":"development/testing/#configuration","title":"Configuration","text":"<p>The molecule configuration files are kept in the role folder you want to test. Create the directory <code>molecule/default</code> and at least the <code>molecule.yml</code> and <code>converge.yml</code>:</p> <pre><code>roles/\n\u2514\u2500\u2500 webserver_demo\n    \u251c\u2500\u2500 defaults\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n    \u251c\u2500\u2500 molecule\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 default\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 converge.yml\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 molecule.yml\n    \u251c\u2500\u2500 tasks\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n    \u2514\u2500\u2500 templates\n        \u2514\u2500\u2500 index.html\n</code></pre> <p>You may use these example configurations as a starting point. It expects that the Container image is already present (use <code>podman pull docker.io/timgrt/rockylinux9-ansible:latest</code>).</p> Central Molecule configurationPlaybook filePreparation stageVerification <p>molecule.yml</p> <pre><code>---\ndriver:\n  name: podman\nplatforms: # (1)!\n  - name: instance1 # (2)!\n    groups: # (3)!\n      - molecule\n      - rocky\n    image: docker.io/timgrt/rockylinux9-ansible:latest # (4)!\n    volumes:\n      - /sys/fs/cgroup:/sys/fs/cgroup:ro\n    command: \"/usr/sbin/init\"\n    pre_build_image: true # (5)!\n    exposed_ports:\n      - 80/tcp\n    published_ports: # (6)!\n      - 8080:80/tcp\nprovisioner:\n  name: ansible\n  options:\n    D: true # (7)!\n  connection_options:\n    ansible_user: ansible # (8)!\n  config_options:\n    defaults:\n      interpreter_python: auto_silent\n      callback_whitelist: profile_tasks, timer, yaml # (9)!\n  inventory:\n    links:\n      group_vars: ../../../../inventory/group_vars/ # (10)!\nscenario: # (11)!\n  create_sequence:\n    - create\n    - prepare\n  converge_sequence:\n    - create\n    - prepare\n    - converge\n  test_sequence:\n    - destroy\n    - create\n    - converge\n    - idempotence\n    - destroy\n  destroy_sequence:\n    - destroy\n</code></pre> <ol> <li>List of hosts to provision by molecule, copy the list item and use a unique name if you want to deploy multiple containers. In the following example one Container with Rocky Linux 8 and one Ubuntu 20.04 container are provisioned.     <pre><code>  - name: rocky8-instance1\n    image: docker.io/timgrt/rockylinux9-ansible:latest\n    volumes:\n      - /sys/fs/cgroup:/sys/fs/cgroup:ro\n    tmpfs:\n      - /run\n      - /tmp\n    command: \"/usr/sbin/init\"\n    pre_build_image: true\n    groups:\n      - molecule\n      - rocky\n  - name: ubuntu2004\n    image: docker.io/timgrt/ubuntu2004-ansible:latest\n    volumes:\n      - /sys/fs/cgroup:/sys/fs/cgroup:ro\n    command: \"/lib/systemd/systemd\"\n    pre_build_image: true\n    groups:\n      - molecule\n      - ubuntu\n</code></pre></li> <li>The name of your container, for better identification you could use e.g. <code>demo.${USER}.molecule</code> which uses your username from environment variable    substitution, showing who deployed the container for what purpose.</li> <li>Additional groups the host should be part of, using a custom <code>molecule</code> group for referencing in <code>converge.yml</code>. If you want your container to inherit variables from group_vars (see inventory.links.group_vars in the provisioner section), add the group(s) to this list.</li> <li>For more information regarding the used container image, see https://hub.docker.com/r/timgrt/rockylinux9-ansible. The image provides a systemd-enabled environment, this ensures you can install and start services with systemctl as in any normal VM. Some more useful images are:<ul> <li>Rocky Linux 8</li> <li>Fedora 39</li> <li>Ubuntu 20.04</li> <li>Debian 10</li> <li>OpenSuse 15</li> </ul> </li> <li>Container image must be present before running Molecule, pull it with <code>podman pull docker.io/timgrt/rockylinux9-ansible:latest</code></li> <li>When running a webserver inside the container (on port 80), this will publish the container port 80 to the host port 8080. Now, you can check the webserver content by using <code>http://localhost:8080</code> (or use the IP of your host).</li> <li>Enables diff mode, set to <code>false</code> if you don't want that.</li> <li>Uses the ansible user to connect to the container (defined in the container image), this way you can test with <code>become</code>. Otherwise you would connect with the root user, most likely this is not what you would do in production.</li> <li>Adds a timer to every task and the overall playbook run, as well as formatting the Ansible output to YAML for better readability. Install necessary collections with <code>ansible-galaxy collection install ansible.posix community.general</code>.</li> <li>If you want your container to inherit variables from group_vars, reference the location of your group_vars (here they are stored in the subfolder inventory of the project, searching begins in the scenario folder defaults). Delete the inventory key and all content if you don't need this.</li> <li>A scenario allows Molecule to test a role in a particular way, these are the stages when executing Molecule. For example, running <code>molecule converge</code> would create a container (if not already created), prepare it (if not already prepared) and run the converge stage/playbook.  </li> </ol> <p>converge.yml</p> <p>The role to test must be defined here, change <code>role-name</code> to the actual name.</p> <pre><code>---\n- name: Converge\n  hosts: molecule\n  become: true\n  roles:\n    - role-name\n</code></pre> <p>prepare.yml</p> <p>Adds an optional preparation stage (referenced by <code>prepare</code> in the scenario definition). For example, if you want to test SSH Key-Pair creation in your container (this is also used by the user module to create SSH keys), install the necessary packages before running the role itself.</p> <pre><code>---\n- name: Prepare\n  hosts: molecule\n  become: true\n  tasks:\n    - name: Install OpenSSH for ssh-keygen\n      ansible.builtin.package:\n        name: openssh\n        state: present\n</code></pre> <p>Remember, you are using a Container image, not every package from the distribution is installed by default to minimize the image size.</p> <p>verify.yml</p> <p>Adds an optional verification stage (referenced by <code>verify</code> in the scenario definition). Not used in the example above.</p> <p>Add this block to your <code>molecule.yml</code> as a top-level key:</p> <pre><code>verifier:\n  name: ansible\n</code></pre> <p>The <code>verify.yml</code> contains your tests for your role.</p> <pre><code>---\n- name: Verify\n  hosts: molecule\n  become: true\n  tasks:\n    - name: Get service facts\n      ansible.builtin.service_facts:\n\n    # Service may have started, returning 'OK' in the service module, but may have failed later.\n    - name: Ensure that MariaDB is in running state\n      assert:\n        that:\n          - ansible_facts['services']['mariadb.service']['state'] == 'running'\n</code></pre> <p>Other verifiers like testinfra can be used.</p>"},{"location":"development/testing/#usage","title":"Usage","text":"<p>Molecule is executed from within the role you want to test, change directory:</p> <pre><code>cd roles/webserver_demo\n</code></pre> <p>From here, run the molecule scenario, after activating your Python VE with molecule:</p> <pre><code>source molecule-venv/bin/activate\n</code></pre> <p>To only create the defined containers, but not run the Ansible tasks:</p> <pre><code>molecule create\n</code></pre> <p>To run the Ansible tasks of the role (if the container does not exist, it will be created):</p> <pre><code>molecule converge\n</code></pre> <p>To execute a full test circle (existing containers are deleted, re-created and Ansible tasks are executed, containers are deleted(!) afterwards):</p> <pre><code>molecule test\n</code></pre> <p>If you want to login to a running container instance:</p> <pre><code>molecule login\n</code></pre>"},{"location":"development/testing/#minimal-testing-environment","title":"Minimal testing environment","text":"<p>Tip</p> <p>This is meant as a quick and dirty testing or demo environment only, for anything more sophisticated, use Molecule (as you most likely will be moving your content into one or more roles anyway).</p> <p>You'll miss out on the convenient and frankly easy to use possibilities of Molecule, but, if you just need a small environment for testing your Ansible content without impacting your Ansible Control Node, the following setup spins up a small one in (Podman) containers. You will need Podman and Ansible (naturally), but nothing else.</p>"},{"location":"development/testing/#installation_1","title":"Installation","text":"<p>You can install Podman with the following command:</p> <pre><code>sudo apt install podman\n</code></pre> <p>The playbook to create the testing instances uses the containers.podman collection, if you only installed <code>ansible-core</code>, you'll need to install the collection separately:</p> <pre><code>ansible-galaxy collection install containers.podman\n</code></pre>"},{"location":"development/testing/#configuration_1","title":"Configuration","text":"<p>Copy the three files in the separate tabs, a playbook for creating the testing environment, an inventory file defining the testing instances and a small demo playbook which can be used to test your Ansible content.</p> Create test environmentTesting inventoryTesting Playbook <p>testing_environment.yml</p> <pre><code>---\n- name: Create or delete demo environment for local testing\n  hosts: localhost\n  connection: local\n  vars:\n    testing_image: docker.io/timgrt/rockylinux9-ansible:latest\n  tasks:\n    - name: \"{{ (delete | default(false)) | ternary('Delete', 'Create') }} demo instance\"\n      containers.podman.podman_container:\n        name: \"{{ item }}\"\n        hostname: \"{{ item }}\"\n        image: \"{{ testing_image }}\"\n        volumes:\n          - /sys/fs/cgroup:/sys/fs/cgroup:ro\n        command: \"/usr/sbin/init\"\n        state: \"{{ (delete | default(false)) | ternary('absent', 'started') }}\"\n      loop: \"{{ groups['test'] }}\"\n</code></pre> <p>testing_inventory.yml</p> <p>Add additional instances in the <code>test</code> group, if necessary.</p> <pre><code>[test]\ninstance1\n\n[test:vars]\nansible_user=ansible\nansible_connection=podman\n</code></pre> <p>testing_inventory.yml</p> <p>Add your tasks to this playbook and start testing. If you want to use your own playbook, target the <code>test</code> group as well.</p> <pre><code>---\n- name: Testing playbook\n  hosts: test\n  tasks:\n    - name: Output distribution\n      ansible.builtin.debug:\n        msg: \"{{ ansible_distribution }}\"\n</code></pre>"},{"location":"development/testing/#usage_1","title":"Usage","text":"<p>First, create the testing instances by executing the <code>testing_environment.yml</code> playbook:</p> <pre><code>ansible-playbook -i testing_inventory.ini testing_environment.yml\n</code></pre> <p>Add your tasks to the <code>testing_playbook.yml</code> (or use your existing playbook, target the <code>test</code> group) and execute:</p> <pre><code>ansible-playbook -i testing_inventory.ini testing_playbook.yml\n</code></pre> <p>After finishing your tests remove the instances by running the <code>testing_environment.yml</code> playbook and provide the extra-var <code>delete</code>:</p> <pre><code>ansible-playbook -i testing_inventory.ini testing_environment.yml -e delete=true\n</code></pre>"},{"location":"mindset/","title":"The Zen of Ansible","text":"<p>Your Ansible automation content doesn\u2019t necessarily have to follow this guidance, but they\u2019re good ideas to keep in mind. These aphorisms are opinions that can be debated and sometimes can be contradictory. What matters is that they communicate a mindset for getting the most from Ansible and your automation.</p> <ul> <li> <p> 20 aphorisms for Ansible</p> <pre><code>Ansible is not Python.\nYAML sucks for coding.\nPlaybooks are not for programming.\nAnsible users are (most likely) not programmers.\nClear is better than cluttered.\nConcise is better than verbose.\nSimple is better than complex.\nReadability counts.\nHelping users get things done matters most.\nUser experience beats ideological purity.\n\u201cMagic\u201d conquers the manual.\nWhen giving users options, use convention over configuration.\nDeclarative is better than imperative \u2013 most of the time.\nFocus avoids complexity.\nComplexity kills productivity.\nIf the implementation is hard to explain, it's a bad idea.\nEvery shell command and UI interaction is an opportunity to automate.\nJust because something works, doesn\u2019t mean it can\u2019t be improved.\nFriction should be eliminated whenever possible.\nAutomation is a journey that never ends.\n</code></pre> </li> </ul> <p>Let me take you deeper into each of the aphorisms and explain what they mean to your automation practice.</p>"},{"location":"mindset/#ansible-is-not-python","title":"Ansible is not Python","text":"<p>YAML sucks for coding. Playbooks are not for programming. Ansible users are (most probably) not programmers.  </p> <p>These aphorisms are at the heart of why applying guidelines for a programming language to good Ansible automation content didn\u2019t seem right to me. As I said, it would give the wrong impression and would reinforce a mindset we don't recommend \u2013 that Ansible is a programming language for coding your playbooks.</p> <p>These aphorisms are all saying the same thing in different ways \u2013 certainly the first 3. If you're trying to \"write code\" in your plays and roles, you're setting yourself up for failure. Ansible\u2019s YAML-based playbooks were never meant to be for programming.</p> <p>So it bothers me when I see Python-isms bleeding into what Ansible users see and do. It may be natural and make sense if you write code in Python, but most Ansible users are not Pythonistas. So, it can be challenging and confusing when these isms are incorporated, thereby introducing friction that degrades their user experience and the value that Ansible provides.</p> <p>By Ansible not being a programming language, all parts of your organization can contribute to automating your entire IT stack rather than relying on skill programmers to understand your operations to write and maintain code for it.</p> <p>If you are a programmer creating Ansible modules and plugins, assume you are not the target audience for what you are developing and your target audience won\u2019t have the same skills and resources you possess.</p>"},{"location":"mindset/#clear-concise-simple","title":"Clear, Concise, Simple","text":"<p>Clear is better than cluttered. Concise is better than verbose. Simple is better than complex. Readability counts.  </p> <p>These are really just interpretations of aphorisms in \u201cThe Zen of Python\u201d. The last one is taken directly from it because you can\u2019t improve on perfection.</p> <p>In the original Ansible best practices talk, we recommended users optimize for readability. This holds true even more so today. If done properly, your content can be the documentation of your workflow automation. Take the time to make your automation as clear and concise as possible. Iterate over what you create and always look for opportunities to simplify and clarify.</p> <p>These aphorisms don\u2019t just apply to those writing playbooks and creating roles. If you are a module developer, think about how your work can assist users, be clear and concise, do things simply and just get things done.</p>"},{"location":"mindset/#helping-users","title":"Helping users","text":"<p>Helping users get things done matters most. User experience beats ideological purity.  </p> <p>Whether you are creating modules, plugins and collections or writing playbooks or designing a cross domain hybrid automation workflow \u2013 Ansible is for helping you get things done. Always consider and look to maximize the user experience. Don\u2019t get caught up and beholden to some strict interpretation of standards or ideological purity that shifts the burden on the end user.</p>"},{"location":"mindset/#its-a-kind-of-magic","title":"It's a kind of Magic","text":"<p>\u201cMagic\u201d conquers the manual Arthur C. Clarke wrote, \u201cAny sufficiently advanced technology is indistinguishable from magic.\u201d</p> <p>The \u201cmagic\u201d in Ansible is its playbook engine and module system. It is how Ansible provides powerful and flexible capabilities in a straightforward and accessible way by abstracting users from all of the complex implementation details that lie beneath. This frees users from doing time consuming and error prone manual operations or writing brittle one-off scripts and code, enabling them the time to put their valuable expertise to use where it is needed.</p> <p>Design automation that amazes users can make difficult or tedious tasks easy and almost effortless. Look to provide powerful time saving capabilities that are quick to deploy and utilize them to get things done.</p>"},{"location":"mindset/#convention-over-configuration","title":"Convention over configuration","text":"<p>When giving users options, use convention over configuration.  </p> <p>I am a big proponent of convention over configuration and don\u2019t think it gets enough consideration in the Ansible community. Convention over configuration is a design paradigm that attempts to decrease the number of decisions that a developer is required to make without necessarily losing flexibility so they don't have to repeat themselves. It was popularized by Ruby on Rails.</p> <p>A playbook developer utilizing your work should only need to specify unique and unconventional aspects of their automation tasks and workflows and no more. Look to reduce the number of decisions and implementation details a user needs to make. Take the time to handle the most common use cases for them. Look to provide as many sensible defaults with modules, plugins and roles as possible. Optimize for users to get things done quickly.</p>"},{"location":"mindset/#declarative","title":"Declarative","text":"<p>Declarative is better than imperative \u2013 most of the time.</p> <p>This aphorism is particularly for Ansible Content Collection developers. Ansible is a desired state engine by design. Think declaratively first. If there truly is no way to design something declaratively, then use imperative (procedural) means.</p> <p>Declarative means that configuration is guaranteed by a set of facts instead of by a set of instructions, for example, \u201cthere should be 10 RHEL servers\u201d, rather than \u201cdepending on how many RHEL servers are running, start/stop servers until you have 10, and tell me if it worked or not\u201d.</p> <p>This aphorism is an example of the \u201cuser experience beats ideological purity\u201d aphorism in practice. Rather than strictly adhering to a declarative approach to automation, Ansible incorporates declarative and imperative means. This mix offers you the flexibility to focus on what you need to do, rather than strictly adhere to one paradigm.</p>"},{"location":"mindset/#avoid-complexity","title":"Avoid complexity","text":"<p>Focus avoids complexity. Complexity kills productivity.</p> <p>Remember that complexity kills productivity. The Ansible team at Red Hat really means it and believes that. That's not just a marketing slogan. Automation can crush complexity and give you the one thing you can\u2019t get enough of \u23af time.</p> <p>Follow Linux principles of doing one thing, and one thing well. Keep roles and playbooks focused on a specific purpose. Multiple simple ones are better than having a huge single playbook full of conditionals and \u201cprogramming\u201d that Ansible is not well suited for.</p> <p>We strive to reduce complexity in how we've designed Ansible and encourage you to do the same. Strive for simplification in what you automate.</p>"},{"location":"mindset/#hard-to-explain","title":"Hard to explain !?","text":"<p>If the implementation is hard to explain, it's a bad idea.  </p> <p>This aphorism, like \u201creadability counts\u201d, is also taken directly from \u201cThe Zen of Python\u201d because you cannot improve upon perfection.</p> <p>In his essay on Literate Programming, Charles Knuth wrote, \u201cInstead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.\u201d So it goes that if you cannot explain or document your implementation easily, then it\u2019s a bad idea that needs to be rethought or scrapped. If it is hard to explain, what chance do others have of understanding it, using it and debugging it? Kernighan\u2019s Law says \u201cDebugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.\u201d</p> <p>Ansible is designed for how real people think and work. Recall earlier when I said Ansible Playbooks are human readable automation with no special coding skills needed. Take advantage of that. Then, if you are having trouble explaining what you are trying to do, pause and re-consider your implementation and the process you are trying to automate. How can I make it easier to explain? Can my process be improved or streamlined? How can I simplify and clarify? Can I break it down into smaller more focused parts and iterate over this?</p> <p>This will help you identify a bad idea sooner and avoid the types of friction that will slow down you and your organization over time.</p>"},{"location":"mindset/#opportunity-to-automate","title":"Opportunity to automate!","text":"<p>Every shell command and UI interaction is an opportunity to automate.</p> <p>This aphorism comes from my personal experience talking about Ansible and automation for many years. Sometimes I am asked what they should automate. Other times, I am challenged that an automation tool like Ansible is unnecessary or does not apply to what they are doing. No matter if we were talking about RHEL, Windows, networking infrastructure, security, edge devices, or cloud services, my response has essentially been the same over the years. I have repeated it so often, that I have jokingly formulated the point into my own theorem on automation. So call it \u201cAppnel's Theorem on Automation\u201d if you will.</p> <p>If you are wondering what should be automated, look for anything anyone is typing into a Linux shell and clicking through in a user interface. Then ask yourself \u201cis this something that can be automated?\u201d Then ask \u201cwhat is the value of automating this?\u201d Most Ansible modules wrap command line tools or use the same APIs behind UIs.</p> <p>Given a sufficient number of things to automate is identified, start with those that cause the most pain and those that you can get done quickly. Remember you want to create a virtuous cycle of releasing reliability, feedback and building trust across your organization. Showing progress and business value quickly will help do that.</p>"},{"location":"mindset/#cant-be-improved","title":"Can't be improved?","text":"<p>Just because something works, doesn\u2019t mean it can\u2019t be improved. Friction should be eliminated whenever possible.  </p> <p>This first aphorism just so happens to be a quote from the movie Black Panther, and it elegantly expresses some important wisdom when it comes to Ansible automation.</p> <p>Always iterate and adapt to real world feedback from your operations. Optimize readability. Continue to find ways to simplify and reduce friction in your organization and its processes. As changes are introduced into your environments and IT policies over time, they will create new friction and pain points. They will also create new opportunities to apply your automation practices to eliminate them.</p>"},{"location":"mindset/#never-ending-story","title":"Never ending story...","text":"<p>Automation is a journey that never ends.</p> <p>Heraclitus, a Greek philosopher, said \"change is the only constant in life. Nothing endures but change.\"</p> <p>Anyone who has been around the IT industry for any length of time knows there is constant change. This is why it is so vital to be agile and prepared to respond to ongoing change, innovation and business demands quickly and reliably.</p> <p>Automation is not a destination. It is a practice. It is a culture, a mindset and an attitude. Automation is a continuous process of feedback and learning and adapting to change and improving upon what you did before.</p> <p>Automation creates opportunities and we at Red Hat see opportunities for automation everywhere.</p> <p>So the question I pose to you is: Where will your automation journey lead you?</p>"},{"location":"mindset/#further-reading","title":"Further Reading","text":"<p>If you want to dive more deeply into the application of the zen of Ansible and its origins, I recommend these resources.</p> <p>The Ansible Community of Practice (CoP) has assembled a comprehensive repository of \u201cgood practices\u201d for Ansible content development. The Ansible Lint tool has now been added to the Red Hat Ansible Automation Platform and codifies many of these practices in rules and profiles to help you quickly identify and enforce consistent application to your work.</p>"},{"location":"mindset/#source","title":"Source","text":"<p>Ansible Blog - The Zen of Ansible</p>"}]}